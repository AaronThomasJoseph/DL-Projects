{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AaronThomasJoseph/DL-Projects/blob/main/CS671_A1_G21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVAUgNyg90gR"
      },
      "outputs": [],
      "source": [
        "#Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7Rlremnrd8I"
      },
      "source": [
        "# **PERCEPTRON**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAg6NthyCJLb"
      },
      "source": [
        "##**`Classification`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXa59FEPbkW3"
      },
      "outputs": [],
      "source": [
        "# plotting the data\n",
        "def Plot(Data,title,nrow,ncol,index):\n",
        "  plt.subplot(nrow,ncol,index)\n",
        "  color = [\"red\",\"green\",\"blue\"]\n",
        "  plt.title(title)\n",
        "  for i in range(len(Data)):\n",
        "    plt.scatter(Data[i].T[0],Data[i].T[1],color=color[i],label=\"Class \"+str(i+1),marker='.')\n",
        "  plt.xlabel(\"x1\")\n",
        "  plt.ylabel(\"x2\")\n",
        "  plt.grid()\n",
        "  plt.legend(loc=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXtE9oRG7HMy"
      },
      "outputs": [],
      "source": [
        "# normalizing the data\n",
        "def Normalization(Data):\n",
        "  Data = (Data - Data.min(axis=0)) / (Data.max(axis=0) - Data.min(axis=0))\n",
        "  return Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiWvNJ5oYrWe"
      },
      "outputs": [],
      "source": [
        "# confusion matrix\n",
        "def Confusion_Matrix(Actual,Predicted):\n",
        "  C = len(set(Actual))\n",
        "  Matrix = np.zeros((C,C),int)\n",
        "\n",
        "  for i in range(1,C+1):\n",
        "    for j in range(1,C+1):\n",
        "      Matrix[i-1][j-1] = sum(Actual[Predicted==j]==i)\n",
        "\n",
        "  Accuracy = 0\n",
        "  Precision= 0\n",
        "  Recall   = 0\n",
        "  F1_score = 0\n",
        "  for i in range(C):\n",
        "      Accuracy += Matrix[i][i]/Matrix.sum()\n",
        "      if sum(Matrix[i]):\n",
        "          Precision+= Matrix[i][i]/sum(Matrix[i])*1/C\n",
        "      if sum(Matrix[:,i]):\n",
        "          Recall+=Matrix[i][i]/sum(Matrix[:,i])*1/C\n",
        "\n",
        "  try:F1_score = 2*(Precision*Recall)/(Precision+Recall)\n",
        "  except:F1_score = 0\n",
        "\n",
        "  print('Confusion Matrix:\\n',Matrix)\n",
        "  print('Accuracy:\\t',np.around(Accuracy,4))\n",
        "  print('Precision:\\t',np.around(Precision,4))\n",
        "  print('Recall:\\t\\t',np.around(Recall,4))\n",
        "  print('F1-Score:\\t',np.around(F1_score,4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGtT_z9uREDT"
      },
      "outputs": [],
      "source": [
        "# activation function for perceptron\n",
        "def Activation_Function(a,beta=1,func=\"Logistic\"):\n",
        "  e = np.e\n",
        "  if func==\"Logistic\":\n",
        "    f = 1/(1+e**(-beta*a))\n",
        "  elif func==\"Tanh\":\n",
        "    f = (e**(beta*a) - e**(-beta*a)) / (e**(beta*a) + e**(-beta*a))\n",
        "  elif func==\"Linear\":\n",
        "    f = a\n",
        "  else:\n",
        "    return \"Wrong Input\"\n",
        "  return f\n",
        "\n",
        "# gradient of activation function for perceptron\n",
        "def Gradient_Activation_Function(a,beta=1,func=\"Logistic\"):\n",
        "  f = Activation_Function(a,beta,func)\n",
        "  if func==\"Logistic\":\n",
        "    df = beta*f*(1-f)\n",
        "  elif func==\"Tanh\":\n",
        "    df = beta*(1-f*f)\n",
        "  elif func==\"Linear\":\n",
        "    df = 1\n",
        "  else:\n",
        "    return \"Wrong Input\"\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9xblvJtKhgH"
      },
      "outputs": [],
      "source": [
        "# decision regions\n",
        "def Decision_Region(C,clf,func,par):\n",
        "  x1 = np.arange(0,1,0.01)\n",
        "  x2 = np.arange(0,1,0.01)\n",
        "  X = np.zeros((len(x1)*len(x2),2))\n",
        "  for i in range(len(x1)):\n",
        "    for j in range(len(x2)):\n",
        "      arr = [x1[i],x2[j]]\n",
        "      X[i*len(x1)+j] = arr\n",
        "  y = np.zeros((len(x1)*len(x2)))\n",
        "  if clf == \"perceptron\":\n",
        "    for i in range(len(X)):\n",
        "        #x = [x0,x1,x2,....]\n",
        "        x = np.array([1,X[i][0],X[i][1]])\n",
        "        Pred = []\n",
        "        #One vs One Approach\n",
        "        l = 0\n",
        "        for j in range(C):\n",
        "          for k in range(j+1,C):\n",
        "            #Activation Value\n",
        "            a = par[l]@x\n",
        "            #Output From Activation Function\n",
        "            beta=1\n",
        "            sn = Activation_Function(a,beta,func)\n",
        "            if func==\"Logistic\":\n",
        "              if   sn>0.5:Pred.append(j+1)\n",
        "              else:       Pred.append(k+1)\n",
        "            elif func==\"Tanh\":\n",
        "              if   sn>0 :Pred.append(j+1)\n",
        "              else:      Pred.append(k+1)\n",
        "            l+=1\n",
        "\n",
        "        lst = Pred\n",
        "        y[i] = max(set(lst), key=lst.count)\n",
        "\n",
        "  elif clf == \"1 layer\":\n",
        "    W1, b1, W2, b2 , Z1, Z2, A1, A2 = par\n",
        "    Z1 = W1.dot(X.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    y = get_predictions(A2)+1\n",
        "\n",
        "  elif clf ==\"2 layer\":\n",
        "    W1, b1, W2, b2, W3, b3 , Z1, Z2, Z3, A1, A2, A3 = par\n",
        "    Z1 = W1.dot(X.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    A3 = softmax(Z3)\n",
        "    y = get_predictions(A3)+1\n",
        "\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.title(\"Decision Region\",fontweight=\"bold\")\n",
        "  Colour = [\"salmon\",\"lightgreen\",\"skyblue\"]\n",
        "  for i in range(C):\n",
        "    x = X[y==i+1]\n",
        "    plt.scatter(x.T[0],x.T[1],color=Colour[i],label=\"Surface \"+str(i+1),alpha=1/4)\n",
        "  plt.xlabel(\"x1\")\n",
        "  plt.ylabel(\"x2\")\n",
        "  plt.grid()\n",
        "  plt.legend()\n",
        "  #plt.tight_layout()\n",
        "  #plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65JYAep1aRDo"
      },
      "outputs": [],
      "source": [
        "def Perceptron_Classification(X_train,X_validation,X_test,y_train,y_validation,y_test,\n",
        "               epoch=1e3,error=1e-5,beta=1,func=\"Logistic\",title=\"Linearly Seperable\"):\n",
        "  #Size of Traininig Data\n",
        "  N = X_train.shape[0]\n",
        "  #Size of Validation Data\n",
        "  M = X_validation.shape[0]\n",
        "  #Dimension of Input Data\n",
        "  d = X_train.shape[1]\n",
        "  #No of Classes\n",
        "  C = len(set(y_train))\n",
        "  #Weight Initialization\n",
        "  w = np.random.random(size=(C*(C-1)//2,d+1))\n",
        "  #Learning Rate\n",
        "  learning_rate=1\n",
        "  #Error\n",
        "  E_train      = 1e5\n",
        "  E_train_prev = 0\n",
        "  #Epoch\n",
        "  itr = 0\n",
        "  #Index\n",
        "  index = list(range(N))\n",
        "  #To store Training and Validation Errors\n",
        "  Training_Error  =[]\n",
        "  Validation_Error=[]\n",
        "  Test_Error  =[]\n",
        "  #Perceptron Algorithm\n",
        "  while((itr<=epoch) and (abs(E_train-E_train_prev)>=error)):\n",
        "    #Shuffling Training Data\n",
        "    np.random.shuffle(index)\n",
        "    X_train = X_train[index]\n",
        "    y_train = y_train[index]\n",
        "\n",
        "    E_train_prev = E_train\n",
        "    E_train      = 0\n",
        "    for i in range(N):\n",
        "      #x = [x0,x1,x2,....]\n",
        "      x = np.concatenate([[1,*X_train[i]]])\n",
        "      y = y_train[i]\n",
        "      #One vs One Approach\n",
        "      l = 0\n",
        "      for j in range(C):\n",
        "        for k in range(j+1,C):\n",
        "          if y==j+1 or y==k+1:\n",
        "            #Activation Value\n",
        "            a = w[l]@x\n",
        "            #Output From Activation Function\n",
        "            sn = Activation_Function(a,beta,func)\n",
        "            if func==\"Logistic\":\n",
        "              if   y==j+1:yn=1\n",
        "              elif y==k+1:yn=0\n",
        "            elif func==\"Tanh\":\n",
        "              if   y==j+1:yn= 1\n",
        "              elif y==k+1:yn=-1\n",
        "\n",
        "            #En\n",
        "            E_train += 0.5*(yn-sn)**2\n",
        "            #ùõø0\n",
        "            delta0 = (yn-sn)*Gradient_Activation_Function(a,beta,func)\n",
        "            #w = w + ‚ñ≥w  = w + Œ∑*ùõø0*x\n",
        "            w[l] += learning_rate*delta0*x\n",
        "          l+=1\n",
        "\n",
        "    E_validation = 0\n",
        "    for i in range(M):\n",
        "      #x = [x0,x1,x2,....]\n",
        "      x = np.concatenate([[1,*X_validation[i]]])\n",
        "      y = y_validation[i]\n",
        "      #One vs One Approach\n",
        "      l = 0\n",
        "      for j in range(C):\n",
        "        for k in range(j+1,C):\n",
        "          if y==j+1 or y==k+1:\n",
        "            #Activation Value\n",
        "            a = w[l]@x\n",
        "            #Output From Activation Function\n",
        "            sn = Activation_Function(a,beta,func)\n",
        "            if func==\"Logistic\":\n",
        "              if   y==j+1:yn=1\n",
        "              elif y==k+1:yn=0\n",
        "            elif func==\"Tanh\":\n",
        "              if   y==j+1:yn= 1\n",
        "              elif y==k+1:yn=-1\n",
        "\n",
        "            E_validation += 0.5*(yn-sn)**2\n",
        "          l+=1\n",
        "\n",
        "    E_test = 0\n",
        "    for i in range(len(X_test)):\n",
        "      #x = [x0,x1,x2,....]\n",
        "      x = np.concatenate([[1,*X_test[i]]])\n",
        "      y = y_test[i]\n",
        "      #One vs One Approach\n",
        "      l = 0\n",
        "      for j in range(C):\n",
        "        for k in range(j+1,C):\n",
        "          if y==j+1 or y==k+1:\n",
        "            #Activation Value\n",
        "            a = w[l]@x\n",
        "            #Output From Activation Function\n",
        "            sn = Activation_Function(a,beta,func)\n",
        "            if func==\"Logistic\":\n",
        "              if   y==j+1:yn=1\n",
        "              elif y==k+1:yn=0\n",
        "            elif func==\"Tanh\":\n",
        "              if   y==j+1:yn= 1\n",
        "              elif y==k+1:yn=-1\n",
        "\n",
        "            E_test += 0.5*(yn-sn)**2\n",
        "          l+=1\n",
        "\n",
        "    learning_rate = 1/(itr+1)**(1/2)\n",
        "    E_train /= N\n",
        "    E_validation /= M\n",
        "    E_test /= len(X_test)\n",
        "    Training_Error.append(E_train)\n",
        "    Validation_Error.append(E_validation)\n",
        "    Test_Error.append(E_test)\n",
        "    itr+=1\n",
        "\n",
        "  #Error Plot\n",
        "  plt.figure(figsize=(12.5,2.5))\n",
        "  plt.suptitle(title,fontweight=\"bold\",y=1+1e-2)\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.title(\"Training Error\")\n",
        "  plt.plot(range(1,len(Training_Error)+1),Training_Error,marker=\".\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Average Error\")\n",
        "  plt.grid()\n",
        "\n",
        "  # Validation error plot\n",
        "  plt.subplot(1,3,2)\n",
        "  plt.title(\"Validation Error\")\n",
        "  plt.plot(range(1,len(Validation_Error)+1),Validation_Error,marker=\".\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Average Error\")\n",
        "  plt.grid()\n",
        "\n",
        "  # Testing error plot\n",
        "  plt.subplot(1,3,3)\n",
        "  plt.title(\"Test Error\")\n",
        "  plt.plot(range(1,len(Test_Error)+1),Test_Error,marker=\".\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Average Error\")\n",
        "  plt.grid()\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # plotting the main points of the data\n",
        "  l = 0\n",
        "  x = np.arange(0,1,0.01)\n",
        "  colour=[\"red\",\"green\",\"blue\"]\n",
        "  plt.figure(figsize=(12.5,(C*(C+1)//2+1)//2*2.5))\n",
        "  for j in range(C):\n",
        "        for k in range(j+1,C):\n",
        "          X_j = X_train[y_train==j+1]\n",
        "          X_k = X_train[y_train==k+1]\n",
        "          plt.subplot((C*(C+1)//2+1)//2,2,l+1)\n",
        "          plt.title(\"Class \"+str(j+1)+\"vs Class \"+str(k+1))\n",
        "          plt.scatter(X_j.T[0],X_j.T[1],color=colour[j],label=\"Class \"+str(j+1),marker='.')\n",
        "          plt.scatter(X_k.T[0],X_k.T[1],color=colour[k],label=\"Class \"+str(k+1),marker='.')\n",
        "          y = -(w[l][1]*x + w[l][0])/w[l][2]\n",
        "          plt.plot(x,y,label=\"Decision\\nBoundary\",color=\"black\",linestyle=\"-.\")\n",
        "          plt.xlabel(\"x1\")\n",
        "          plt.ylabel(\"y1\")\n",
        "          plt.xlim(-0.05,1.05)\n",
        "          plt.ylim(-0.05,1.05)\n",
        "          plt.legend(loc=1,fontsize=\"small\",framealpha=0)\n",
        "          plt.grid()\n",
        "          l+=1\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  Decision_Region(C,clf=\"perceptron\",func=\"Logistic\",par=w)\n",
        "  for k in range(C):\n",
        "    X = X_train[y_train==k+1]\n",
        "    plt.scatter(X.T[0],X.T[1],color=colour[k],label=\"Class \"+str(j+1),marker='.')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  #Prediction\n",
        "  #Training Data\n",
        "  y_train_pred = y_train*0\n",
        "  for i in range(N):\n",
        "    #x = [x0,x1,x2,....]\n",
        "    x = np.concatenate([[1,*X_train[i]]])\n",
        "    Pred = []\n",
        "    #One vs One Approach\n",
        "    l = 0\n",
        "    for j in range(C):\n",
        "      for k in range(j+1,C):\n",
        "        #Activation Value\n",
        "        a = w[l]@x\n",
        "        #Output From Activation Function\n",
        "        sn = Activation_Function(a,beta,func)\n",
        "        if func==\"Logistic\":\n",
        "          if   sn>0.5:Pred.append(j+1)\n",
        "          else:       Pred.append(k+1)\n",
        "        elif func==\"Tanh\":\n",
        "          if   sn>0 :Pred.append(j+1)\n",
        "          else:      Pred.append(k+1)\n",
        "        l+=1\n",
        "\n",
        "    y_train_pred[i] = statistics.mode(Pred)\n",
        "    Pred = []\n",
        "\n",
        "  #Validation Data\n",
        "  y_validation_pred = y_validation*0\n",
        "  for i in range(M):\n",
        "    #x = [x0,x1,x2,....]\n",
        "    x = np.concatenate([[1,*X_validation[i]]])\n",
        "    Pred = []\n",
        "    #One vs One Approach\n",
        "    l = 0\n",
        "    for j in range(C):\n",
        "      for k in range(j+1,C):\n",
        "        #Activation Value\n",
        "        a = w[l]@x\n",
        "        #Output From Activation Function\n",
        "        beta=1\n",
        "        sn = Activation_Function(a,beta,func)\n",
        "        if func==\"Logistic\":\n",
        "          if   sn>0.5:Pred.append(j+1)\n",
        "          else:       Pred.append(k+1)\n",
        "        elif func==\"Tanh\":\n",
        "          if   sn>0 :Pred.append(j+1)\n",
        "          else:      Pred.append(k+1)\n",
        "        l+=1\n",
        "\n",
        "    y_validation_pred[i] = statistics.mode(Pred)\n",
        "    Pred = []\n",
        "\n",
        "  #Test Data\n",
        "  y_test_pred = y_test*0\n",
        "  for i in range(len(X_test)):\n",
        "    #x = [x0,x1,x2,....]\n",
        "    x = np.concatenate([[1,*X_test[i]]])\n",
        "    Pred = []\n",
        "    #One vs One Approach\n",
        "    l = 0\n",
        "    for j in range(C):\n",
        "      for k in range(j+1,C):\n",
        "        #Activation Value\n",
        "        a = w[l]@x\n",
        "        #Output From Activation Function\n",
        "        sn = Activation_Function(a,beta,func)\n",
        "        if func==\"Logistic\":\n",
        "          if   sn>0.5:Pred.append(j+1)\n",
        "          else:       Pred.append(k+1)\n",
        "        elif func==\"Tanh\":\n",
        "          if   sn>0 :Pred.append(j+1)\n",
        "          else:      Pred.append(k+1)\n",
        "        l+=1\n",
        "\n",
        "    y_test_pred[i] = statistics.mode(Pred)\n",
        "    Pred = []\n",
        "\n",
        "  print('*'*80)\n",
        "  print(\" \"*35+\"Training Data\")\n",
        "  print(\" \"*35+\"_____________\")\n",
        "  Confusion_Matrix(y_train,y_train_pred)\n",
        "  print(\"\\n\"+\" \"*35+\"Validation Data\")\n",
        "  print(\" \"*35+\"_______________\")\n",
        "  Confusion_Matrix(y_validation,y_validation_pred)\n",
        "  print(\"\\n\"+\" \"*35+\"Test Data\")\n",
        "  print(\" \"*35+\"_________\")\n",
        "  Confusion_Matrix(y_test,y_test_pred)\n",
        "  print('*'*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTUMAqXZ_xeZ"
      },
      "outputs": [],
      "source": [
        "#DataSet\n",
        "#Linearly Seperable Data\n",
        "LS_C1 = np.loadtxt(\"LS_Class1.txt\")\n",
        "LS_C2 = np.loadtxt(\"LS_Class2.txt\")\n",
        "LS_C3 = np.loadtxt(\"LS_Class3.txt\")\n",
        "\n",
        "#Min_Max Normalization\n",
        "LS = np.concatenate([LS_C1,LS_C2,LS_C3])\n",
        "LS = Normalization(LS)\n",
        "LS_C1,LS_C2,LS_C3 = LS[:len(LS_C1)],LS[len(LS_C1):len(LS_C1)+len(LS_C2)],LS[len(LS_C1)+len(LS_C2):]\n",
        "\n",
        "#Plot\n",
        "plt.figure(figsize=(12,12.5))\n",
        "plt.suptitle(\"Classification Data\",fontweight=\"bold\",y=1+1e-2)\n",
        "Plot(Data=[LS_C1,LS_C2,LS_C3],title=\"Linearly Seperable Data\",nrow=4,ncol=2,index=1)\n",
        "\n",
        "#Train-Validation-Test Split\n",
        "np.random.shuffle(LS_C1)\n",
        "np.random.shuffle(LS_C2)\n",
        "np.random.shuffle(LS_C3)\n",
        "len1 = len(LS_C1)\n",
        "len2 = len(LS_C2)\n",
        "len3  = len(LS_C3)\n",
        "LS_C1_train,LS_C1_validation,LS_C1_test = LS_C1[:int(.60*len1)],LS_C1[int(.60*len1):int(.80*len1)],LS_C1[int(.80*len1):]\n",
        "LS_C2_train,LS_C2_validation,LS_C2_test = LS_C2[:int(.60*len2)],LS_C2[int(.60*len2):int(.80*len2)],LS_C2[int(.80*len2):]\n",
        "LS_C3_train,LS_C3_validation,LS_C3_test = LS_C3[:int(.60*len3)],LS_C3[int(.60*len3):int(.80*len3)],LS_C3[int(.80*len3):]\n",
        "\n",
        "LS_X_train = np.concatenate((LS_C1_train,LS_C2_train,LS_C3_train))\n",
        "LS_y_train = np.concatenate((1*np.ones(len(LS_C1_train)),2*np.ones(len(LS_C2_train)),3*np.ones(len(LS_C3_train))))\n",
        "LS_X_validation = np.concatenate((LS_C1_validation,LS_C2_validation,LS_C3_validation))\n",
        "LS_y_validation = np.concatenate((1*np.ones(len(LS_C1_validation)),2*np.ones(len(LS_C2_validation)),3*np.ones(len(LS_C3_validation))))\n",
        "LS_X_test = np.concatenate((LS_C1_test,LS_C2_test,LS_C3_test))\n",
        "LS_y_test = np.concatenate((1*np.ones(len(LS_C1_test)),2*np.ones(len(LS_C2_test)),3*np.ones(len(LS_C3_test))))\n",
        "\n",
        "#Plot\n",
        "Plot(Data=[LS_C1_train,LS_C2_train,LS_C3_train],title=\"Linearly Seperable Training Data\",nrow=4,ncol=2,index=3)\n",
        "Plot(Data=[LS_C1_validation,LS_C2_validation,LS_C3_validation],title=\"Linearly Seperable Validation Data\",nrow=4,ncol=2,index=5)\n",
        "Plot(Data=[LS_C1_test,LS_C2_test,LS_C3_test],title=\"Linearly Seperable Test Data\",nrow=4,ncol=2,index=7)\n",
        "\n",
        "#Non Linearly Seperable Data\n",
        "NLS = np.loadtxt(\"NLS_Group21.txt\",skiprows=1,)\n",
        "NLS_C1 = NLS[:2446]\n",
        "NLS_C2 = NLS[2446:]\n",
        "\n",
        "#Min-Max Normalization\n",
        "NLS = Normalization(NLS)\n",
        "NLS_C1,NLS_C2 = NLS[:len(NLS_C1)],NLS[len(NLS_C1):]\n",
        "\n",
        "#Plot\n",
        "Plot(Data=[NLS_C1,NLS_C2],title=\"Non Linearly Seperable Data\",nrow=4,ncol=2,index=2)\n",
        "\n",
        "#Train-Validation-Test Split\n",
        "np.random.shuffle(NLS_C1)\n",
        "np.random.shuffle(NLS_C2)\n",
        "len1 = len(NLS_C1)\n",
        "len2 = len(NLS_C2)\n",
        "NLS_C1_train,NLS_C1_validation,NLS_C1_test = NLS_C1[:int(.60*len1)],NLS_C1[int(.60*len1):int(.80*len1)],NLS_C1[int(.80*len1):]\n",
        "NLS_C2_train,NLS_C2_validation,NLS_C2_test = NLS_C2[:int(.60*len2)],NLS_C2[int(.60*len2):int(.80*len2)],NLS_C2[int(.80*len2):]\n",
        "\n",
        "NLS_X_train = np.concatenate((NLS_C1_train,NLS_C2_train))\n",
        "NLS_y_train = np.concatenate((1*np.ones(len(NLS_C1_train)),2*np.ones(len(NLS_C2_train))))\n",
        "NLS_X_validation = np.concatenate((NLS_C1_validation,NLS_C2_validation))\n",
        "NLS_y_validation = np.concatenate((1*np.ones(len(NLS_C1_validation)),2*np.ones(len(NLS_C2_validation))))\n",
        "NLS_X_test = np.concatenate((NLS_C1_test,NLS_C2_test))\n",
        "NLS_y_test = np.concatenate((1*np.ones(len(NLS_C1_test)),2*np.ones(len(NLS_C2_test))))\n",
        "\n",
        "#Plot\n",
        "Plot(Data=[NLS_C1_train,NLS_C2_train],title=\"Non Linearly Seperable Training Data\",nrow=4,ncol=2,index=4)\n",
        "Plot(Data=[NLS_C1_validation,NLS_C2_validation],title=\"Non Linearly Seperable Validation Data\",nrow=4,ncol=2,index=6)\n",
        "Plot(Data=[NLS_C1_test,NLS_C2_test],title=\"Non Linearly Seperable Test Data\",nrow=4,ncol=2,index=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#Perceptron Classification\n",
        "#Linearly Seperable Data\n",
        "Perceptron_Classification(LS_X_train,LS_X_validation,LS_X_test,LS_y_train,LS_y_validation,LS_y_test,title=\"Linearly Seperable Data\")\n",
        "\n",
        "#Non Linearly Seperable Data\n",
        "Perceptron_Classification(NLS_X_train,NLS_X_validation,NLS_X_test,NLS_y_train,NLS_y_validation,NLS_y_test,title=\"Non Linearly Seperable Data\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NncSdFghQ1Zd"
      },
      "source": [
        "##**`Regression`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivQTo5kEQ1Av"
      },
      "outputs": [],
      "source": [
        "def Perceptron_Regression(X_train,X_validation,X_test,y_train,y_validation,y_test,\n",
        "               epoch=1e3,error=1e-5,beta=1,func=\"Linear\",title=\"Linearly Seperable\"):\n",
        "  #Size of Traininig Data\n",
        "  N = X_train.shape[0]\n",
        "  #Size of Validation Data\n",
        "  M = X_validation.shape[0]\n",
        "  #Dimension of Input Data\n",
        "  try:d = X_train.shape[1]\n",
        "  except:d=1\n",
        "  #Weight Initialization\n",
        "  w = np.random.random(size=(d+1))\n",
        "  #Learning Rate\n",
        "  learning_rate=1\n",
        "  #Error\n",
        "  E_train      = 1e5\n",
        "  E_train_prev = 0\n",
        "  #Epoch\n",
        "  itr = 0\n",
        "  #Index\n",
        "  index = list(range(N))\n",
        "  #To store Training and Validation Errors\n",
        "  Training_Error  =[]\n",
        "  Validation_Error=[]\n",
        "  Test_Error      =[]\n",
        "  #Perceptron Algorithm\n",
        "  while((itr<=epoch) and (abs(E_train-E_train_prev)>=error)):\n",
        "    #Shuffling Training Data\n",
        "    np.random.shuffle(index)\n",
        "    X_train = X_train[index]\n",
        "    y_train = y_train[index]\n",
        "\n",
        "    E_train_prev = E_train\n",
        "    E_train      = 0\n",
        "    for i in range(N):\n",
        "      #x = [x0,x1,x2,....]\n",
        "      try:x = np.concatenate([[1,*X_train[i]]])\n",
        "      except:x = np.array([1,X_train[i]])\n",
        "      y = y_train[i]\n",
        "\n",
        "      #Activation Value\n",
        "      a = w@x\n",
        "      #Output From Activation Function\n",
        "      #Activation Function is the Linear Function\n",
        "      sn = Activation_Function(a,beta,func)\n",
        "\n",
        "      #En\n",
        "      E_train += 0.5*(y-sn)**2\n",
        "      #ùõø0\n",
        "      delta0 = (y-sn)*Gradient_Activation_Function(a,beta,func)\n",
        "      #w = w + ‚ñ≥w  = w + Œ∑*ùõø0*x\n",
        "      w += learning_rate*delta0*x\n",
        "\n",
        "\n",
        "    E_validation = 0\n",
        "    for i in range(M):\n",
        "      #x = [x0,x1,x2,....]\n",
        "      try:x = np.concatenate([[1,*X_validation[i]]])\n",
        "      except:x = np.array([1,X_validation[i]])\n",
        "      y = y_validation[i]\n",
        "\n",
        "      #Activation Value\n",
        "      a = w@x\n",
        "      #Output From Activation Function\n",
        "      sn = Activation_Function(a,beta,func)\n",
        "\n",
        "      E_validation += 0.5*(y-sn)**2\n",
        "\n",
        "    E_test = 0\n",
        "    for i in range(len(X_test)):\n",
        "      #x = [x0,x1,x2,....]\n",
        "      try:x = np.concatenate([[1,*X_test[i]]])\n",
        "      except:x = np.array([1,X_test[i]])\n",
        "      y = y_test[i]\n",
        "\n",
        "      #Activation Value\n",
        "      a = w@x\n",
        "      #Output From Activation Function\n",
        "      sn = Activation_Function(a,beta,func)\n",
        "\n",
        "      E_test += 0.5*(y-sn)**2\n",
        "\n",
        "    learning_rate = 1/(itr+1)**(1/2)\n",
        "    E_train /= 2*N\n",
        "    E_validation /= 2*M\n",
        "    E_test /= 2*len(X_test)\n",
        "    Training_Error.append(E_train)\n",
        "    Validation_Error.append(E_validation)\n",
        "    Test_Error.append(E_test)\n",
        "    itr+=1\n",
        "\n",
        "  #Error Plot\n",
        "  plt.figure(figsize=(12,5))\n",
        "  plt.suptitle(title,fontweight=\"bold\",y=1+1e-2)\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.title(\"Training Error\")\n",
        "  plt.plot(range(1,len(Training_Error)+1),Training_Error,marker=\".\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Average Error\")\n",
        "  plt.grid()\n",
        "\n",
        "  plt.subplot(1,3,2)\n",
        "  plt.title(\"Validation Error\")\n",
        "  plt.plot(range(1,len(Validation_Error)+1),Validation_Error,marker=\".\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Average Error\")\n",
        "  plt.grid()\n",
        "  #plt.tight_layout()\n",
        "\n",
        "\n",
        "  plt.subplot(1,3,3)\n",
        "  plt.title(\"Test Error\")\n",
        "  plt.plot(range(1,len(Test_Error)+1),Test_Error,marker=\".\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Average Error\")\n",
        "  plt.grid()\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(12,5))\n",
        "  plt.suptitle(title,fontweight=\"bold\",y=1+1e-2)\n",
        "  bar_x = (\"Training Data\",\"Validation Data\",\"Test Data\")\n",
        "  bar_y = (Training_Error[-1],Validation_Error[-1],Test_Error[-1])\n",
        "  plt.bar(bar_x,bar_y)\n",
        "  plt.xlabel(\"Data\")\n",
        "  plt.ylabel(\"Mean Square Error\")\n",
        "  plt.grid()\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  #Prediction\n",
        "  #Training Data\n",
        "  y_train_pred = y_train*0\n",
        "  for i in range(N):\n",
        "    #x = [x0,x1,x2,....]\n",
        "    try:x = np.concatenate([[1,*X_train[i]]])\n",
        "    except:x = np.array([1,X_train[i]])\n",
        "    #Activation Value\n",
        "    a = w@x\n",
        "    #Output From Activation Function\n",
        "    sn = Activation_Function(a,beta,func)\n",
        "    y_train_pred[i] = sn\n",
        "\n",
        "  #Validation Data\n",
        "  y_validation_pred = y_validation*0\n",
        "  for i in range(M):\n",
        "    #x = [x0,x1,x2,....]\n",
        "    try:x = np.concatenate([[1,*X_validation[i]]])\n",
        "    except:x = np.array([1,X_validation[i]])\n",
        "    #Activation Value\n",
        "    a = w@x\n",
        "    #Output From Activation Function\n",
        "    sn = Activation_Function(a,beta,func)\n",
        "    y_validation_pred[i] = sn\n",
        "\n",
        "  #Test Data\n",
        "  y_test_pred = y_test*0\n",
        "  for i in range(len(X_test)):\n",
        "    #x = [x0,x1,x2,....]\n",
        "    try:x = np.concatenate([[1,*X_test[i]]])\n",
        "    except:x = np.array([1,X_test[i]])\n",
        "    #Activation Value\n",
        "    a = w@x\n",
        "    #Output From Activation Function\n",
        "    sn = Activation_Function(a,beta,func)\n",
        "    y_test_pred[i] = sn\n",
        "\n",
        "  #Actual vs Predicted Output\n",
        "  plt.figure(figsize=(12,5))\n",
        "  plt.suptitle(title,fontweight=\"bold\",y=1+1e-2)\n",
        "  #Training Data\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.title(\"Training Data\")\n",
        "  plt.scatter(y_train,y_train_pred,marker=\".\")\n",
        "  plt.xlabel(\"Target Output\")\n",
        "  plt.ylabel(\"Model Output\")\n",
        "  plt.grid()\n",
        "  #Validation Data\n",
        "  plt.subplot(1,3,2)\n",
        "  plt.title(\"Validation Data\")\n",
        "  plt.scatter(y_validation,y_validation_pred,marker=\".\")\n",
        "  plt.xlabel(\"Target Output\")\n",
        "  plt.ylabel(\"Model Output\")\n",
        "  plt.grid()\n",
        "  #Test Data\n",
        "  plt.subplot(1,3,3)\n",
        "  plt.title(\"Training Data\")\n",
        "  plt.scatter(y_test,y_test_pred,marker=\".\")\n",
        "  plt.xlabel(\"Target Output\")\n",
        "  plt.ylabel(\"Model Output\")\n",
        "  plt.grid()\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return (X_train,X_validation,X_test),(y_train,y_validation,y_test),(y_train_pred,y_validation_pred,y_test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pbTxYgDSXLE"
      },
      "outputs": [],
      "source": [
        "#DataSet\n",
        "#Univariate Data\n",
        "Univariate_Data = pd.read_csv(\"UnivariateData.csv\",header=None).values\n",
        "\n",
        "#Plot\n",
        "plt.figure(figsize=(12,12.5))\n",
        "plt.suptitle(\"Regression Data\",fontweight=\"bold\",y=1+1e-2)\n",
        "plt.subplot(4,2,1)\n",
        "plt.title(\"Univariate Data\")\n",
        "plt.scatter(Univariate_Data.T[0],Univariate_Data.T[1])\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid()\n",
        "\n",
        "#Train-Validation-Test Split\n",
        "np.random.shuffle(Univariate_Data)\n",
        "len_univariate  = len(Univariate_Data)\n",
        "Univariate_train,Univariate_validation,Univariate_test = Univariate_Data[:int(.60*len_univariate)],Univariate_Data[int(.60*len_univariate):int(.80*len_univariate)],Univariate_Data[int(.80*len_univariate):]\n",
        "\n",
        "\n",
        "#Plot\n",
        "plt.subplot(4,2,3)\n",
        "plt.title(\"Univariate Training Data\")\n",
        "plt.scatter(Univariate_train.T[0],Univariate_train.T[1])\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(4,2,5)\n",
        "plt.title(\"Univariate Validation Data\")\n",
        "plt.scatter(Univariate_validation.T[0],Univariate_validation.T[1])\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(4,2,7)\n",
        "plt.title(\"Univariate Test Data\")\n",
        "plt.scatter(Univariate_test.T[0],Univariate_test.T[1])\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid()\n",
        "\n",
        "#Bivariate Data\n",
        "Bivariate_Data = pd.read_csv(\"BivariateData.csv\",header=None).values\n",
        "\n",
        "#Train-Validation-Test Split\n",
        "np.random.shuffle(Bivariate_Data)\n",
        "len_bivariate  = len(Bivariate_Data)\n",
        "Bivariate_train,Bivariate_validation,Bivariate_test = Bivariate_Data[:int(.60*len_bivariate)],Bivariate_Data[int(.60*len_bivariate):int(.80*len_bivariate)],Bivariate_Data[int(.80*len_bivariate):]\n",
        "\n",
        "#Plot\n",
        "ax = plt.subplot(4,2,2,projection=\"3d\")\n",
        "plt.title(\"Bivariate Data\")\n",
        "ax.scatter3D(*Bivariate_Data.T)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "ax.set_zlabel(\"y\")\n",
        "plt.grid()\n",
        "\n",
        "ax = plt.subplot(4,2,4,projection=\"3d\")\n",
        "plt.title(\"Bivariate Training Data\")\n",
        "ax.scatter3D(*Bivariate_train.T)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "ax.set_zlabel(\"y\")\n",
        "plt.grid()\n",
        "\n",
        "ax = plt.subplot(4,2,6,projection=\"3d\")\n",
        "plt.title(\"Bivariate Validation Data\")\n",
        "ax.scatter3D(*Bivariate_validation.T)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "ax.set_zlabel(\"y\")\n",
        "plt.grid()\n",
        "\n",
        "ax = plt.subplot(4,2,8,projection=\"3d\")\n",
        "plt.title(\"Bivariate Test Data\")\n",
        "ax.scatter3D(*Bivariate_test.T)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "ax.set_zlabel(\"y\")\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#Perceptron Regression\n",
        "#Univariate Data\n",
        "Univariate_X,Univariate_y,Univariate_y_pred = Perceptron_Regression(Univariate_train.T[0],Univariate_validation.T[0],Univariate_test.T[0],\n",
        "                                            Univariate_train.T[-1],Univariate_validation.T[-1],Univariate_test.T[-1],title=\"Univariate Data\")\n",
        "#Bivariate Data\n",
        "Bivariate_X,Bivariate_y,Bivariate_y_pred = Perceptron_Regression(Bivariate_train.T[:2].T,Bivariate_validation.T[:2].T,Bivariate_test.T[:2].T,\n",
        "                                            Bivariate_train.T[-1],Bivariate_validation.T[-1],Bivariate_test.T[-1],title=\"Bivariate Data\")\n",
        "\n",
        "#Plot\n",
        "plt.figure(figsize=(12,12.5))\n",
        "plt.suptitle(\"Actual vs Predicted Output\",fontweight=\"bold\",y=1+1e-2)\n",
        "plt.subplot(3,2,1)\n",
        "plt.title(\"Univariate Train Data\")\n",
        "plt.scatter(Univariate_X[0],Univariate_y[0],label='Actual',alpha=1/2)\n",
        "plt.scatter(Univariate_X[0],Univariate_y_pred[0],label='Prediction',alpha=1/2)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "\n",
        "plt.subplot(3,2,3)\n",
        "plt.title(\"Univariate Validation Data\")\n",
        "plt.scatter(Univariate_X[1],Univariate_y[1],label='Actual',alpha=1/2)\n",
        "plt.scatter(Univariate_X[1],Univariate_y_pred[1],label='Prediction',alpha=1/2)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "\n",
        "plt.subplot(3,2,5)\n",
        "plt.title(\"Univariate Test Data\")\n",
        "plt.scatter(Univariate_X[2],Univariate_y[2],label='Actual',alpha=1/2)\n",
        "plt.scatter(Univariate_X[2],Univariate_y_pred[2],label='Prediction',alpha=1/2)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "\n",
        "ax = plt.subplot(3,2,2,projection=\"3d\")\n",
        "plt.title(\"Bivariate Train Data\")\n",
        "ax.scatter3D(*Bivariate_X[0].T,Bivariate_y[0],label='Actual',alpha=1/2)\n",
        "ax.scatter3D(*Bivariate_X[0].T,Bivariate_y_pred[0],label='Prediction',alpha=1/2)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "ax.set_zlabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "\n",
        "ax = plt.subplot(3,2,4,projection=\"3d\")\n",
        "plt.title(\"Bivariate Validation Data\")\n",
        "ax.scatter3D(*Bivariate_X[1].T,Bivariate_y[1],label='Actual',alpha=1/2)\n",
        "ax.scatter3D(*Bivariate_X[1].T,Bivariate_y_pred[1],label='Prediction',alpha=1/2)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "ax.set_zlabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "\n",
        "ax = plt.subplot(3,2,6,projection=\"3d\")\n",
        "plt.title(\"Bivariate Test Data\")\n",
        "ax.scatter3D(*Bivariate_X[2].T,Bivariate_y[2],label='Actual',alpha=1/2)\n",
        "ax.scatter3D(*Bivariate_X[2].T,Bivariate_y_pred[2],label='Prediction',alpha=1/2)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "ax.set_zlabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q5lTwOUrwa6"
      },
      "source": [
        "# **FCNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90cfaZq6P6NA"
      },
      "source": [
        "##Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg_L7e0GHmdP"
      },
      "source": [
        "### 1 hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asiIroZqHbMU"
      },
      "outputs": [],
      "source": [
        "def init_params(d,C,nodes):\n",
        "    W1 = np.random.normal(0,1,(nodes, d))\n",
        "    b1 = np.random.normal(0,1,(nodes, 1))\n",
        "    W2 = np.random.normal(0,1,(C, nodes))\n",
        "    b2 = np.random.normal(0,1,(C, 1))\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def ReLU(Z):\n",
        "    return np.maximum(Z, 0)\n",
        "\n",
        "def softmax(Z):\n",
        "    A = np.exp(Z) / sum(np.exp(Z))\n",
        "    return A\n",
        "\n",
        "def forward_prop(W1, b1, W2, b2, X):\n",
        "    X = np.reshape(X, (2, 1))\n",
        "    Z1 = W1.dot(X) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "def ReLU_deriv(Z):\n",
        "    return Z > 0\n",
        "\n",
        "def one_hot(Y,C):\n",
        "    try:\n",
        "      vec = np.zeros(C,int)\n",
        "      vec[int(Y)-1] = 1\n",
        "    except:\n",
        "      vec = np.zeros((len(Y),C),int)\n",
        "      for i in range(C):\n",
        "        vec[Y==i+1,i] = 1\n",
        "    return vec.T\n",
        "\n",
        "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y,d,C,nodes):\n",
        "    one_hot_Y = one_hot(Y,C)\n",
        "    one_hot_Y = np.reshape(one_hot_Y, (C, 1))\n",
        "    X = np.reshape(X,(d,1))\n",
        "    dZ2 = A2 - one_hot_Y\n",
        "    dZ2 = np.reshape(dZ2, (C, 1))\n",
        "    dW2 = dZ2.dot(A1.T)\n",
        "    dW2 = np.reshape(dW2, (C, nodes))\n",
        "    db2 = np.sum(dZ2)\n",
        "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
        "    dW1 = dZ1.dot(X.T)\n",
        "    db1 = np.sum(dZ1)\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
        "    W1 = W1 - alpha * dW1\n",
        "    b1 = b1 - alpha * db1\n",
        "    W2 = W2 - alpha * dW2\n",
        "    b2 = b2 - alpha * db2\n",
        "\n",
        "    return W1, b1, W2, b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPmNLLgdHct_"
      },
      "outputs": [],
      "source": [
        "def get_predictions(A2):\n",
        "    return np.argmax(A2, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    print(predictions, Y)\n",
        "    return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "def gradient_descent(X, Y,X_validation,Y_validation,X_test,Y_test,iterations,alpha,Error,nodes,title):\n",
        "    m = X.shape[0]\n",
        "    d = X.shape[1]\n",
        "    C = len(set(Y))\n",
        "    Error_train      = []\n",
        "    Error_validation = []\n",
        "    Error_test       = []\n",
        "    W1, b1, W2, b2 = init_params(d,C,nodes)\n",
        "    for i in range(int(iterations)):\n",
        "      for j in range(m):\n",
        "        k = np.random.randint(0,m-1)\n",
        "        tempX = X[k]\n",
        "        tempY = Y[k]\n",
        "\n",
        "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, tempX)\n",
        "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, tempX, tempY,d,C,nodes)\n",
        "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
        "\n",
        "\n",
        "      Z1 = W1.dot(X.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = softmax(Z2)\n",
        "      y = one_hot(Y,C)\n",
        "      Error_train.append(((A2-y)**2).sum() / m)\n",
        "      predictions_train = get_predictions(A2)\n",
        "\n",
        "      Z1 = W1.dot(X_validation.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = softmax(Z2)\n",
        "      y = one_hot(Y_validation,C)\n",
        "      Error_validation.append(((A2-y)**2).sum() / m)\n",
        "      predictions_validation = get_predictions(A2)\n",
        "\n",
        "      Z1 = W1.dot(X_test.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = softmax(Z2)\n",
        "      y = one_hot(Y_test,C)\n",
        "      Error_test.append(((A2-y)**2).sum() / m)\n",
        "      predictions_test = get_predictions(A2)\n",
        "\n",
        "      if i>1:\n",
        "        if abs(Error_train[-1]-Error_train[-2]) and abs(Error_train[-1]-Error_train[-3]) <Error:\n",
        "          break\n",
        "    #Decision Region\n",
        "    Decision_Region(C,clf=\"1 layer\",func=None,par=(W1, b1, W2, b2 , Z1, Z2, A1, A2))\n",
        "    Colour=[\"red\",\"green\",\"blue\"]\n",
        "    for k in range(C):\n",
        "      x = X[Y==k+1]\n",
        "      plt.scatter(x.T[0],x.T[1],color=Colour[k],label=\"Class \"+str(j+1),marker='.')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    #Error Plot\n",
        "    plt.figure(figsize=(12.5,5))\n",
        "    plt.suptitle(title,fontweight=\"bold\",y=1+1e-2)\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"Training Error\")\n",
        "    plt.plot(range(1,len(Error_train )+1),Error_train ,marker=\".\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Average Error\")\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Validation Error\")\n",
        "    plt.plot(range(1,len(Error_validation)+1),Error_validation,marker=\".\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Average Error\")\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.title(\"Test Error\")\n",
        "    plt.plot(range(1,len(Error_test)+1),Error_test,marker=\".\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Average Error\")\n",
        "    plt.grid()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print('*'*80)\n",
        "    print(\" \"*35+\"Training Data\")\n",
        "    print(\" \"*35+\"_____________\")\n",
        "    Confusion_Matrix(Y,predictions_train+1)\n",
        "    print(\"\\n\"+\" \"*35+\"Validation Data\")\n",
        "    print(\" \"*35+\"_______________\")\n",
        "    Confusion_Matrix(Y_validation,predictions_validation+1)\n",
        "    print(\"\\n\"+\" \"*35+\"Test Data\")\n",
        "    print(\" \"*35+\"_________\")\n",
        "    Confusion_Matrix(Y_test,predictions_test+1)\n",
        "    print('*'*80)\n",
        "\n",
        "    #Plot Hidden Layer\n",
        "    plt.figure(figsize=(12.5,4*nodes))\n",
        "    plt.suptitle(title+\"\\nHidden Layer\",fontweight=\"bold\",y=1+1e-2)\n",
        "\n",
        "    # plots of hidden nodes\n",
        "    p = 1\n",
        "    Z1 = W1.dot(X.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    Colour=[\"red\",\"green\",\"blue\"]\n",
        "    for j in range(nodes):\n",
        "      ax = plt.subplot(nodes,3,p,projection=\"3d\")\n",
        "      plt.title(\"Training Data\\nHidden Node\"+str(j+1))\n",
        "      for i in range(C):\n",
        "        x  = X[Y==i+1]\n",
        "        a1 = A1[j,Y==i+1]\n",
        "        ax.scatter3D(x.T[0],x.T[1],a1,color=Colour[i],label=\"Class \"+str(i+1))\n",
        "      plt.xlabel(\"x1\")\n",
        "      plt.ylabel(\"x2\")\n",
        "      ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "      plt.grid()\n",
        "      plt.legend(loc=1,fontsize=\"small\")\n",
        "      p+=3\n",
        "\n",
        "    p=2\n",
        "    Z1 = W1.dot(X_validation.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    Colour=[\"red\",\"green\",\"blue\"]\n",
        "    for j in range(nodes):\n",
        "      ax = plt.subplot(nodes,3,p,projection=\"3d\")\n",
        "      plt.title(\"Validation Data\\nHidden Node\"+str(j+1))\n",
        "      for i in range(C):\n",
        "        x  = X_validation[Y_validation==i+1]\n",
        "        a1 = A1[j,Y_validation==i+1]\n",
        "        ax.scatter3D(x.T[0],x.T[1],a1,color=Colour[i],label=\"Class \"+str(i+1))\n",
        "      plt.xlabel(\"x1\")\n",
        "      plt.ylabel(\"x2\")\n",
        "      ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "      plt.grid()\n",
        "      plt.legend(loc=1,fontsize=\"small\")\n",
        "      p+=3\n",
        "\n",
        "    p=3\n",
        "    Z1 = W1.dot(X_test.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    Colour=[\"red\",\"green\",\"blue\"]\n",
        "    for j in range(nodes):\n",
        "      ax = plt.subplot(nodes,3,p,projection=\"3d\")\n",
        "      plt.title(\"Test Data\\nHidden Node\"+str(j+1))\n",
        "      for i in range(C):\n",
        "        x  = X_test[Y_test==i+1]\n",
        "        a1 = A1[j,Y_test==i+1]\n",
        "        ax.scatter3D(x.T[0],x.T[1],a1,color=Colour[i],label=\"Class \"+str(i+1))\n",
        "      plt.xlabel(\"x1\")\n",
        "      plt.ylabel(\"x2\")\n",
        "      ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "      plt.grid()\n",
        "      plt.legend(loc=1,fontsize=\"small\")\n",
        "      p+=3\n",
        "    #plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    #Plot Output Layer\n",
        "    plt.figure(figsize=(12.5,4*C))\n",
        "    plt.suptitle(title+\"\\nOutput Layer\",fontweight=\"bold\",y=1+1e-2)\n",
        "\n",
        "    p = 1\n",
        "    Z1 = W1.dot(X.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    Colour=[\"red\",\"green\",\"blue\"]\n",
        "    for j in range(C):\n",
        "      ax = plt.subplot(C,3,p,projection=\"3d\")\n",
        "      plt.title(\"Training Data\\nOutput Node\"+str(j+1))\n",
        "      for i in range(C):\n",
        "        x  = X[Y==i+1]\n",
        "        a2 = A2[j,Y==i+1]\n",
        "        ax.scatter3D(x.T[0],x.T[1],a2,color=Colour[i],label=\"Class \"+str(i+1))\n",
        "      plt.xlabel(\"x1\")\n",
        "      plt.ylabel(\"x2\")\n",
        "      ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "      plt.grid()\n",
        "      plt.legend(loc=1,fontsize=\"small\")\n",
        "      p+=3\n",
        "\n",
        "    p=2\n",
        "    Z1 = W1.dot(X_validation.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    Colour=[\"red\",\"green\",\"blue\"]\n",
        "    for j in range(C):\n",
        "      ax = plt.subplot(C,3,p,projection=\"3d\")\n",
        "      plt.title(\"Validation Data\\nOutput Node\"+str(j+1))\n",
        "      for i in range(C):\n",
        "        x  = X_validation[Y_validation==i+1]\n",
        "        a2 = A2[j,Y_validation==i+1]\n",
        "        ax.scatter3D(x.T[0],x.T[1],a2,color=Colour[i],label=\"Class \"+str(i+1))\n",
        "      plt.xlabel(\"x1\")\n",
        "      plt.ylabel(\"x2\")\n",
        "      ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "      plt.grid()\n",
        "      plt.legend(loc=1,fontsize=\"small\")\n",
        "      p+=3\n",
        "\n",
        "    p=3\n",
        "    Z1 = W1.dot(X_test.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    Colour=[\"red\",\"green\",\"blue\"]\n",
        "    for j in range(C):\n",
        "      ax = plt.subplot(C,3,p,projection=\"3d\")\n",
        "      plt.title(\"Test Data\\nOutput Node\"+str(j+1))\n",
        "      for i in range(C):\n",
        "        x  = X_test[Y_test==i+1]\n",
        "        a2 = A2[j,Y_test==i+1]\n",
        "        ax.scatter3D(x.T[0],x.T[1],a2,color=Colour[i],label=\"Class \"+str(i+1))\n",
        "      plt.xlabel(\"x1\")\n",
        "      plt.ylabel(\"x2\")\n",
        "      ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "      plt.grid()\n",
        "      plt.legend(loc=1,fontsize=\"small\")\n",
        "      p+=3\n",
        "    #plt.tight_layout()\n",
        "    plt.show()\n",
        "    return W1, b1, W2, b2 , Z1, Z2, A1, A2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHZdxwtLrOFq"
      },
      "outputs": [],
      "source": [
        "W1, b1, W2, b2 , Z1, Z2, A1, A2 = gradient_descent(LS_X_train, LS_y_train,LS_X_validation,LS_y_validation,LS_X_test,LS_y_test,\n",
        "                                  alpha=0.01,Error=1e-5,iterations=1e3,nodes=3,title=\"Linear Seperable Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX__f41vHih4"
      },
      "outputs": [],
      "source": [
        "W1, b1, W2, b2 , Z1, Z2, A1, A2 = gradient_descent(NLS_X_train, NLS_y_train,NLS_X_validation,NLS_y_validation,NLS_X_test,NLS_y_test,\n",
        "                                  alpha=0.01,Error=1e-5,iterations=1e3,nodes=100,title=\"Non Linear Seperable Data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNauR6BqAATK"
      },
      "source": [
        "### 2 hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGdnRmMlALad"
      },
      "outputs": [],
      "source": [
        "def init_params(d,C,node1,node2):\n",
        "    W1 = np.random.normal(0,1,(node1, d))\n",
        "    b1 = np.random.normal(0,1,(node1, 1))\n",
        "    W2 = np.random.normal(0,1,(node2,node1))\n",
        "    b2 = np.random.normal(0,1,(node2, 1))\n",
        "    W3 = np.random.normal(0,1,(C, node2))\n",
        "    b3 = np.random.normal(0,1,(C, 1))\n",
        "    return W1, b1, W2, b2, W3, b3\n",
        "\n",
        "def forward_prop(W1, b1, W2, b2, W3, b3, X):\n",
        "    X = np.reshape(X, (2, 1))\n",
        "    Z1 = W1.dot(X) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    A3 = softmax(Z3)\n",
        "    return Z1, A1, Z2, A2, Z3, A3\n",
        "\n",
        "\n",
        "\n",
        "def backward_prop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y,C,d,node1,node2):\n",
        "\n",
        "    one_hot_Y = one_hot(Y,C)\n",
        "    one_hot_Y = np.reshape(one_hot_Y, (C, 1))\n",
        "    X = np.reshape(X,(d,1))\n",
        "\n",
        "    dZ3 = A3 - one_hot_Y\n",
        "    dZ3 = np.reshape(dZ3, (C, 1))\n",
        "    dW3 = dZ3.dot(A2.T)\n",
        "    dW3 = np.reshape(dW3, (C, node2))\n",
        "    db3 = np.sum(dZ3)\n",
        "\n",
        "    dZ2 = W3.T.dot(dZ3) * ReLU_deriv(Z2)\n",
        "    dZ2 = np.reshape(dZ2, (node2, 1))\n",
        "    dW2 = dZ2.dot(A1.T)\n",
        "    dW2 = np.reshape(dW2, (node2, node1))\n",
        "    db2 = np.sum(dZ2)\n",
        "\n",
        "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
        "    dW1 = dZ1.dot(X.T)\n",
        "    db1 = np.sum(dZ1)\n",
        "    return dW1, db1, dW2, db2, dW3, db3\n",
        "\n",
        "def update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha):\n",
        "    W1 = W1 - alpha * dW1\n",
        "    b1 = b1 - alpha * db1\n",
        "    W2 = W2 - alpha * dW2\n",
        "    b2 = b2 - alpha * db2\n",
        "    W3 = W3 - alpha * dW3\n",
        "    b3 = b3 - alpha * db3\n",
        "    return W1, b1, W2, b2, W3, b3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTMPUOMQAObb"
      },
      "outputs": [],
      "source": [
        "def get_predictions(A3):\n",
        "    return np.argmax(A3, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "def gradient_descent(X, Y,X_validation,Y_validation,X_test,Y_test,iterations,alpha,Error,node1,node2,title):\n",
        "      m = X.shape[0]\n",
        "      d = X.shape[1]\n",
        "      C = len(set(Y))\n",
        "      Error_train      = []\n",
        "      Error_validation = []\n",
        "      Error_test       = []\n",
        "      W1, b1, W2, b2, W3, b3 = init_params(d,C,node1,node2)\n",
        "      for i in range(int(iterations)):\n",
        "        for j in range(m):\n",
        "          k = np.random.randint(0,m-1)\n",
        "          tempX = X[k]\n",
        "          tempY = Y[k]\n",
        "\n",
        "          Z1, A1, Z2, A2, Z3, A3 = forward_prop(W1, b1, W2, b2, W3, b3, tempX)\n",
        "\n",
        "          dW1, db1, dW2, db2, dW3, db3 = backward_prop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, tempX, tempY,C,d,node1,node2)\n",
        "          W1, b1, W2, b2, W3, b3 = update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha)\n",
        "\n",
        "        Z1 = W1.dot(X.T) + b1\n",
        "        A1 = ReLU(Z1)\n",
        "        Z2 = W2.dot(A1) + b2\n",
        "        A2 = ReLU(Z2)\n",
        "        Z3 = W3.dot(A2) + b3\n",
        "        A3 = softmax(Z3)\n",
        "        y = one_hot(Y,C)\n",
        "        Error_train.append(((A3-y)**2).sum() / len(X))\n",
        "        predictions_train = get_predictions(A3)+1\n",
        "\n",
        "        Z1 = W1.dot(X_validation.T) + b1\n",
        "        A1 = ReLU(Z1)\n",
        "        Z2 = W2.dot(A1) + b2\n",
        "        A2 = ReLU(Z2)\n",
        "        Z3 = W3.dot(A2) + b3\n",
        "        A3 = softmax(Z3)\n",
        "        y = one_hot(Y_validation,C)\n",
        "        Error_validation.append(((A3-y)**2).sum() / len(X_validation))\n",
        "        predictions_validation = get_predictions(A3)+1\n",
        "\n",
        "        Z1 = W1.dot(X_test.T) + b1\n",
        "        A1 = ReLU(Z1)\n",
        "        Z2 = W2.dot(A1) + b2\n",
        "        A2 = ReLU(Z2)\n",
        "        Z3 = W3.dot(A2) + b3\n",
        "        A3 = softmax(Z3)\n",
        "        y = one_hot(Y_test,C)\n",
        "        Error_test.append(((A3-y)**2).sum() / len(X_test))\n",
        "        predictions_test = get_predictions(A3)+1\n",
        "\n",
        "        if i>1:\n",
        "          if abs(Error_train[-1]-Error_train[-2]) and abs(Error_train[-1]-Error_train[-3]) <Error:\n",
        "            break\n",
        "      #Decision Region\n",
        "      Decision_Region(C,clf=\"2 layer\",func=None,par=(W1, b1, W2, b2,W3,b3 , Z1, Z2,Z3, A1, A2,A3))\n",
        "      Colour=[\"red\",\"green\",\"blue\"]\n",
        "      for k in range(C):\n",
        "        x = X[Y==k+1]\n",
        "        plt.scatter(x.T[0],x.T[1],color=Colour[k],label=\"Class \"+str(j+1),marker='.')\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "      #Error Plot\n",
        "      plt.figure(figsize=(12,5))\n",
        "      plt.suptitle(title,fontweight=\"bold\",y=1+1e-2)\n",
        "      plt.subplot(1,3,1)\n",
        "      plt.title(\"Training Error\")\n",
        "      plt.plot(range(1,len(Error_train )+1),Error_train ,marker=\".\")\n",
        "      plt.xlabel(\"Epochs\")\n",
        "      plt.ylabel(\"Average Error\")\n",
        "      plt.grid()\n",
        "\n",
        "      plt.subplot(1,3,2)\n",
        "      plt.title(\"Validation Error\")\n",
        "      plt.plot(range(1,len(Error_validation)+1),Error_validation,marker=\".\")\n",
        "      plt.xlabel(\"Epochs\")\n",
        "      plt.ylabel(\"Average Error\")\n",
        "      plt.grid()\n",
        "\n",
        "      plt.subplot(1,3,3)\n",
        "      plt.title(\"Test Error\")\n",
        "      plt.plot(range(1,len(Error_test)+1),Error_test,marker=\".\")\n",
        "      plt.xlabel(\"Epochs\")\n",
        "      plt.ylabel(\"Average Error\")\n",
        "      plt.grid()\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "      #print(predictions_train)\n",
        "      #print(predictions_validation)\n",
        "      #print(predictions_test)\n",
        "      print('*'*80)\n",
        "      print(\" \"*35+\"Training Data\")\n",
        "      print(\" \"*35+\"_____________\")\n",
        "      Confusion_Matrix(Y,predictions_train)\n",
        "      print(\"\\n\"+\" \"*35+\"Validation Data\")\n",
        "      print(\" \"*35+\"_______________\")\n",
        "      Confusion_Matrix(Y_validation,predictions_validation)\n",
        "      print(\"\\n\"+\" \"*35+\"Test Data\")\n",
        "      print(\" \"*35+\"_________\")\n",
        "      Confusion_Matrix(Y_test,predictions_test)\n",
        "      print('*'*80)\n",
        "\n",
        "      #Plot Hidden Layer1\n",
        "      plt.figure(figsize=(12.5,4*node1))\n",
        "      plt.suptitle(title+\"\\nHidden Layer1\",fontweight=\"bold\",y=1+1e-2)\n",
        "\n",
        "      p = 1\n",
        "      Z1 = W1.dot(X.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = ReLU(Z2)\n",
        "      Z3 = W3.dot(A2) + b3\n",
        "      A3 = softmax(Z3)\n",
        "      Colour=[\"red\",\"green\",\"blue\"]\n",
        "      for j in range(node1):\n",
        "        ax = plt.subplot(node1,3,p,projection=\"3d\")\n",
        "        plt.title(\"Training Data\\nHidden Node\"+str(j+1))\n",
        "        for i in range(C):\n",
        "          x  = X[Y==i+1]\n",
        "          a1 = A1[j,Y==i+1]\n",
        "          ax.scatter3D(x.T[0],x.T[1],a1,color=Colour[i],label=\"Class \"+str(i+1))\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        plt.legend(loc=1,fontsize=\"small\")\n",
        "        p+=3\n",
        "\n",
        "      p=2\n",
        "      Z1 = W1.dot(X_validation.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = ReLU(Z2)\n",
        "      Z3 = W3.dot(A2) + b3\n",
        "      A3 = softmax(Z3)\n",
        "      Colour=[\"red\",\"green\",\"blue\"]\n",
        "      for j in range(node1):\n",
        "        ax = plt.subplot(node1,3,p,projection=\"3d\")\n",
        "        plt.title(\"Validation Data\\nHidden Node\"+str(j+1))\n",
        "        for i in range(C):\n",
        "          x  = X_validation[Y_validation==i+1]\n",
        "          a1 = A1[j,Y_validation==i+1]\n",
        "          ax.scatter3D(x.T[0],x.T[1],a1,color=Colour[i],label=\"Class \"+str(i+1))\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        plt.legend(loc=1,fontsize=\"small\")\n",
        "        p+=3\n",
        "\n",
        "      p=3\n",
        "      Z1 = W1.dot(X_test.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = ReLU(Z2)\n",
        "      Z3 = W3.dot(A2) + b3\n",
        "      A3 = softmax(Z3)\n",
        "      Colour=[\"red\",\"green\",\"blue\"]\n",
        "      for j in range(node1):\n",
        "        ax = plt.subplot(node1,3,p,projection=\"3d\")\n",
        "        plt.title(\"Test Data\\nHidden Node\"+str(j+1))\n",
        "        for i in range(C):\n",
        "          x  = X_test[Y_test==i+1]\n",
        "          a1 = A1[j,Y_test==i+1]\n",
        "          ax.scatter3D(x.T[0],x.T[1],a1,color=Colour[i],label=\"Class \"+str(i+1))\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        plt.legend(loc=1,fontsize=\"small\")\n",
        "        p+=3\n",
        "      #plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "      #Plot Hidden Layer2\n",
        "      plt.figure(figsize=(12.5,4*node2))\n",
        "      plt.suptitle(title+\"\\nHidden Layer2\",fontweight=\"bold\",y=1+1e-2)\n",
        "\n",
        "      p = 1\n",
        "      Z1 = W1.dot(X.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = ReLU(Z2)\n",
        "      Z3 = W3.dot(A2) + b3\n",
        "      A3 = softmax(Z3)\n",
        "      Colour=[\"red\",\"green\",\"blue\"]\n",
        "      for j in range(node2):\n",
        "        ax = plt.subplot(node2,3,p,projection=\"3d\")\n",
        "        plt.title(\"Training Data\\nHidden Node\"+str(j+1))\n",
        "        for i in range(C):\n",
        "          x  = X[Y==i+1]\n",
        "          a2 = A2[j,Y==i+1]\n",
        "          ax.scatter3D(x.T[0],x.T[1],a2,color=Colour[i],label=\"Class \"+str(i+1))\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        plt.legend(loc=1,fontsize=\"small\")\n",
        "        p+=3\n",
        "\n",
        "      p=2\n",
        "      Z1 = W1.dot(X_validation.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = ReLU(Z2)\n",
        "      Z3 = W3.dot(A2) + b3\n",
        "      A3 = softmax(Z3)\n",
        "      Colour=[\"red\",\"green\",\"blue\"]\n",
        "      for j in range(node2):\n",
        "        ax = plt.subplot(node2,3,p,projection=\"3d\")\n",
        "        plt.title(\"Validation Data\\nHidden Node\"+str(j+1))\n",
        "        for i in range(C):\n",
        "          x  = X_validation[Y_validation==i+1]\n",
        "          a2 = A2[j,Y_validation==i+1]\n",
        "          ax.scatter3D(x.T[0],x.T[1],a2,color=Colour[i],label=\"Class \"+str(i+1))\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        plt.legend(loc=1,fontsize=\"small\")\n",
        "        p+=3\n",
        "\n",
        "      p=3\n",
        "      Z1 = W1.dot(X_test.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = ReLU(Z2)\n",
        "      Z3 = W3.dot(A2) + b3\n",
        "      A3 = softmax(Z3)\n",
        "      Colour=[\"red\",\"green\",\"blue\"]\n",
        "      for j in range(node2):\n",
        "        ax = plt.subplot(node2,3,p,projection=\"3d\")\n",
        "        plt.title(\"Test Data\\nHidden Node\"+str(j+1))\n",
        "        for i in range(C):\n",
        "          x  = X_test[Y_test==i+1]\n",
        "          a2 = A2[j,Y_test==i+1]\n",
        "          ax.scatter3D(x.T[0],x.T[1],a2,color=Colour[i],label=\"Class \"+str(i+1))\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        plt.legend(loc=1,fontsize=\"small\")\n",
        "        p+=3\n",
        "      #plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "      #Plot Output Layer\n",
        "      plt.figure(figsize=(12.5,4*C))\n",
        "      plt.suptitle(title+\"\\nOutput Layer\",fontweight=\"bold\",y=1+1e-2)\n",
        "\n",
        "      p = 1\n",
        "      Z1 = W1.dot(X.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = ReLU(Z2)\n",
        "      Z3 = W3.dot(A2) + b3\n",
        "      A3 = softmax(Z3)\n",
        "      Colour=[\"red\",\"green\",\"blue\"]\n",
        "      for j in range(C):\n",
        "        ax = plt.subplot(C,3,p,projection=\"3d\")\n",
        "        plt.title(\"Training Data\\nOutput Node\"+str(j+1))\n",
        "        for i in range(C):\n",
        "          x  = X[Y==i+1]\n",
        "          a3 = A3[j,Y==i+1]\n",
        "          ax.scatter3D(x.T[0],x.T[1],a3,color=Colour[i],label=\"Class \"+str(i+1))\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        plt.legend(loc=1,fontsize=\"small\")\n",
        "        p+=3\n",
        "\n",
        "      p=2\n",
        "      Z1 = W1.dot(X_validation.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = ReLU(Z2)\n",
        "      Z3 = W3.dot(A2) + b3\n",
        "      A3 = softmax(Z3)\n",
        "      Colour=[\"red\",\"green\",\"blue\"]\n",
        "      for j in range(C):\n",
        "        ax = plt.subplot(C,3,p,projection=\"3d\")\n",
        "        plt.title(\"Validation Data\\nOutput Node\"+str(j+1))\n",
        "        for i in range(C):\n",
        "          x  = X_validation[Y_validation==i+1]\n",
        "          a3 = A3[j,Y_validation==i+1]\n",
        "          ax.scatter3D(x.T[0],x.T[1],a3,color=Colour[i],label=\"Class \"+str(i+1))\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        plt.legend(loc=1,fontsize=\"small\")\n",
        "        p+=3\n",
        "\n",
        "      p=3\n",
        "      Z1 = W1.dot(X_test.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = ReLU(Z2)\n",
        "      Z3 = W3.dot(A2) + b3\n",
        "      A3 = softmax(Z3)\n",
        "      Colour=[\"red\",\"green\",\"blue\"]\n",
        "      for j in range(C):\n",
        "        ax = plt.subplot(C,3,p,projection=\"3d\")\n",
        "        plt.title(\"Test Data\\nOutput Node\"+str(j+1))\n",
        "        for i in range(C):\n",
        "          x  = X_test[Y_test==i+1]\n",
        "          a3 = A3[j,Y_test==i+1]\n",
        "          ax.scatter3D(x.T[0],x.T[1],a3,color=Colour[i],label=\"Class \"+str(i+1))\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        plt.legend(loc=1,fontsize=\"small\")\n",
        "        p+=3\n",
        "      #plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "      return W1, b1, W2, b2, W3, b3 , Z1, Z2, Z3, A1, A2, A3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdXjZ4-o5iwr"
      },
      "outputs": [],
      "source": [
        "W1, b1, W2, b2, W3, b3 , Z1, Z2, Z3, A1, A2, A3 = gradient_descent(LS_X_train, LS_y_train,LS_X_validation,LS_y_validation,LS_X_test,LS_y_test,\n",
        "                                  alpha=0.01,Error=1e-5,iterations=1e3,node1=10,node2=5,title=\"Non Linear Seperable Data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ0vB-K9ARZz"
      },
      "outputs": [],
      "source": [
        "W1, b1, W2, b2, W3, b3 , Z1, Z2, Z3, A1, A2, A3 = gradient_descent(NLS_X_train, NLS_y_train,NLS_X_validation,NLS_y_validation,NLS_X_test,NLS_y_test,\n",
        "                                  alpha=0.001,Error=1e-5,iterations=1e2,node1=100,node2=50,title=\"Non Linear Seperable Data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leLhwrCGQNG9"
      },
      "source": [
        "##Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLEJDMRCQUcu"
      },
      "source": [
        "###1 hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wjgs2nM1QS8V"
      },
      "outputs": [],
      "source": [
        "def init_params(d,nodes):\n",
        "    W1 = np.random.normal(0,1,(nodes, d))\n",
        "    b1 = np.random.normal(0,1,(nodes, 1))\n",
        "    W2 = np.random.normal(0,1,(1, nodes))\n",
        "    b2 = np.random.normal(0,1,(1, 1))\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def ReLU(Z):\n",
        "    return np.maximum(Z, 0)\n",
        "\n",
        "def linear(Z):\n",
        "    return Z\n",
        "\n",
        "def forward_prop(W1, b1, W2, b2, X,d):\n",
        "    X = np.reshape(X, (d,1))\n",
        "    Z1 = W1.dot(X) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = linear(Z2)\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "def ReLU_deriv(Z):\n",
        "    return Z > 0\n",
        "\n",
        "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y,d,nodes):\n",
        "    X = np.reshape(X,(d,1))\n",
        "    Y = np.reshape(Y,(1,1))\n",
        "    A2 = np.reshape(A2,(1,1))\n",
        "    dZ2 = A2 - Y\n",
        "    dZ2 = np.reshape(dZ2, (1, 1))\n",
        "    dW2 = dZ2.dot(A1.T)\n",
        "    dW2 = np.reshape(dW2, (1, nodes))\n",
        "    db2 = np.sum(dZ2)\n",
        "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
        "    dW1 = dZ1.dot(X.T)\n",
        "    dW1 = np.reshape(dW1,(nodes,d))\n",
        "    db1 = np.sum(dZ1)\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
        "    W1 = W1 - alpha * dW1\n",
        "    b1 = b1 - alpha * db1\n",
        "    W2 = W2 - alpha * dW2\n",
        "    b2 = b2 - alpha * db2\n",
        "\n",
        "    return W1, b1, W2, b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKRB2AWWSY3l"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, Y,X_validation,Y_validation,X_test,Y_test,iterations,alpha,Error,nodes,title):\n",
        "    m = X.shape[0]\n",
        "    try:d = X.shape[1]\n",
        "    except: d = 1\n",
        "    Error_train      = []\n",
        "    Error_validation = []\n",
        "    Error_test       = []\n",
        "    W1, b1, W2, b2 = init_params(d,nodes)\n",
        "    for i in range(int(iterations)):\n",
        "      for j in range(m):\n",
        "        k = np.random.randint(0,m-1)\n",
        "        tempX = X[k]\n",
        "        tempY = Y[k]\n",
        "\n",
        "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, tempX,d)\n",
        "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, tempX, tempY,d,nodes)\n",
        "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
        "\n",
        "\n",
        "      X1 = np.reshape(X,(m,d))\n",
        "      Z1 = W1.dot(X1.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = linear(Z2)\n",
        "      A2 = A2.reshape(m)\n",
        "      y_train_pred = A2\n",
        "      Error_train.append(((A2-Y)**2).sum()/len(X1))\n",
        "\n",
        "      X1 = np.reshape(X_validation,(len(X_validation),d))\n",
        "      Z1 = W1.dot(X1.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = linear(Z2)\n",
        "      A2 = A2.reshape(len(X_validation))\n",
        "      y_validation_pred = A2\n",
        "      Error_validation.append(((A2-Y_validation)**2).sum()/len(X1))\n",
        "\n",
        "      X1 = np.reshape(X_test,(len(X_test),d))\n",
        "      Z1 = W1.dot(X1.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = linear(Z2)\n",
        "      A2 = A2.reshape(len(X_test))\n",
        "      y_test_pred = A2\n",
        "      Error_test.append(((A2-Y_test)**2).sum()/len(X1))\n",
        "\n",
        "      if i>1:\n",
        "        if abs(Error_train[-1]-Error_train[-2]) and abs(Error_train[-1]-Error_train[-3]) <Error:\n",
        "          break\n",
        "    #Error Plot\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.suptitle(title,fontweight=\"bold\",y=1+1e-2)\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"Training Error\")\n",
        "    plt.plot(range(1,len(Error_train)+1),Error_train,marker=\".\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Average Error\")\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Validation Error\")\n",
        "    plt.plot(range(1,len(Error_validation)+1),Error_validation,marker=\".\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Average Error\")\n",
        "    plt.grid()\n",
        "    #plt.tight_layout()\n",
        "\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.title(\"Test Error\")\n",
        "    plt.plot(range(1,len(Error_test)+1),Error_test,marker=\".\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Average Error\")\n",
        "    plt.grid()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.suptitle(title,fontweight=\"bold\",y=1+1e-2)\n",
        "    bar_x = (\"Training Data\",\"Validation Data\",\"Test Data\")\n",
        "    bar_y = (Error_train[-1],Error_validation[-1],Error_test[-1])\n",
        "    plt.bar(bar_x,bar_y)\n",
        "    plt.xlabel(\"Data\")\n",
        "    plt.ylabel(\"Mean Square Error\")\n",
        "    plt.grid()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    #Actual vs Predicted Output\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.suptitle(title,fontweight=\"bold\",y=1+1e-2)\n",
        "    #Training Data\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"Training Data\")\n",
        "    plt.scatter(Y,y_train_pred,marker=\".\")\n",
        "    plt.xlabel(\"Target Output\")\n",
        "    plt.ylabel(\"Model Output\")\n",
        "    plt.grid()\n",
        "    #Validation Data\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Validation Data\")\n",
        "    plt.scatter(Y_validation,y_validation_pred,marker=\".\")\n",
        "    plt.xlabel(\"Target Output\")\n",
        "    plt.ylabel(\"Model Output\")\n",
        "    plt.grid()\n",
        "    #Test Data\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.title(\"Training Data\")\n",
        "    plt.scatter(Y_test,y_test_pred,marker=\".\")\n",
        "    plt.xlabel(\"Target Output\")\n",
        "    plt.ylabel(\"Model Output\")\n",
        "    plt.grid()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    #Plot Hidden Layer\n",
        "    plt.figure(figsize=(12.5,4*nodes))\n",
        "    plt.suptitle(title+\"\\nHidden Layer\",fontweight=\"bold\",y=1+1e-2)\n",
        "\n",
        "    p = 1\n",
        "    X1 = np.reshape(X,(m,d))\n",
        "    Z1 = W1.dot(X1.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = linear(Z2)\n",
        "    A2 = A2.reshape(len(X1))\n",
        "    for j in range(nodes):\n",
        "      if d==2:\n",
        "        ax = plt.subplot(nodes,3,p,projection=\"3d\")\n",
        "        plt.title(\"Training Data\\nHidden Node\"+str(j+1))\n",
        "        ax.scatter3D(X1.T[0],X1.T[1],A1[j])\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "      elif d==1:\n",
        "        plt.subplot(nodes,3,p)\n",
        "        plt.title(\"Training Data\\nHidden Node\"+str(j+1))\n",
        "        plt.scatter(X1,A1[j])\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "\n",
        "    p = 2\n",
        "    X1 = np.reshape(X_validation,(len(X_validation),d))\n",
        "    Z1 = W1.dot(X1.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = linear(Z2)\n",
        "    A2 = A2.reshape(len(X1))\n",
        "    for j in range(nodes):\n",
        "      if d==2:\n",
        "        ax = plt.subplot(nodes,3,p,projection=\"3d\")\n",
        "        plt.title(\"Validation Data\\nHidden Node\"+str(j+1))\n",
        "        ax.scatter3D(X1.T[0],X1.T[1],A1[j])\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "      elif d==1:\n",
        "        plt.subplot(nodes,3,p)\n",
        "        plt.title(\"Validation Data\\nHidden Node\"+str(j+1))\n",
        "        plt.scatter(X1,A1[j])\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "\n",
        "    p = 3\n",
        "    X1 = np.reshape(X_test,(len(X_test),d))\n",
        "    Z1 = W1.dot(X1.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = linear(Z2)\n",
        "    A2 = A2.reshape(len(X1))\n",
        "    for j in range(nodes):\n",
        "      if d==2:\n",
        "        ax = plt.subplot(nodes,3,p,projection=\"3d\")\n",
        "        plt.title(\"Test Data\\nHidden Node\"+str(j+1))\n",
        "        ax.scatter3D(X1.T[0],X1.T[1],A1[j])\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "      elif d==1:\n",
        "        plt.subplot(nodes,3,p)\n",
        "        plt.title(\"Test Data\\nHidden Node\"+str(j+1))\n",
        "        plt.scatter(X1,A1[j])\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    #Plot Output Layer\n",
        "    nodes=1\n",
        "    plt.figure(figsize=(12.5,5*nodes))\n",
        "    plt.suptitle(title+\"\\nOutput Layer\",fontweight=\"bold\",y=1+1e-2)\n",
        "\n",
        "    p = 1\n",
        "    X1 = np.reshape(X,(m,d))\n",
        "    Z1 = W1.dot(X1.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = linear(Z2)\n",
        "    A2 = A2.reshape(len(X1))\n",
        "    for j in range(nodes):\n",
        "      if d==2:\n",
        "        ax = plt.subplot(nodes,3,p,projection=\"3d\")\n",
        "        plt.title(\"Training Data\\nOutput Node\"+str(j+1))\n",
        "        ax.scatter3D(X1.T[0],X1.T[1],A2)\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "      elif d==1:\n",
        "        plt.subplot(nodes,3,p)\n",
        "        plt.title(\"Training Data\\nOutput Node\"+str(j+1))\n",
        "        plt.scatter(X1,A2)\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "\n",
        "    p = 2\n",
        "    X1 = np.reshape(X_validation,(len(X_validation),d))\n",
        "    Z1 = W1.dot(X1.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = linear(Z2)\n",
        "    A2 = A2.reshape(len(X1))\n",
        "    for j in range(nodes):\n",
        "      if d==2:\n",
        "        ax = plt.subplot(nodes,3,p,projection=\"3d\")\n",
        "        plt.title(\"Validation Data\\nOutput Node\"+str(j+1))\n",
        "        ax.scatter3D(X1.T[0],X1.T[1],A2)\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "      elif d==1:\n",
        "        plt.subplot(nodes,3,p)\n",
        "        plt.title(\"Validation Data\\nOutput Node\"+str(j+1))\n",
        "        plt.scatter(X1,A2)\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "\n",
        "    p = 3\n",
        "    X1 = np.reshape(X_test,(len(X_test),d))\n",
        "    Z1 = W1.dot(X1.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = linear(Z2)\n",
        "    A2 = A2.reshape(len(X1))\n",
        "    for j in range(nodes):\n",
        "      if d==2:\n",
        "        ax = plt.subplot(nodes,3,p,projection=\"3d\")\n",
        "        plt.title(\"Test Data\\nOutput Node\"+str(j+1))\n",
        "        ax.scatter3D(X1.T[0],X1.T[1],A2)\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "      elif d==1:\n",
        "        plt.subplot(nodes,3,p)\n",
        "        plt.title(\"Test Data\\nOutput Node\"+str(j+1))\n",
        "        plt.scatter(X1,A2)\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return (y_train_pred,y_validation_pred,y_test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tezszk46Sr2H"
      },
      "outputs": [],
      "source": [
        "Univariate_y_pred = gradient_descent(Univariate_X[0], Univariate_y[0],Univariate_X[1], Univariate_y[1],Univariate_X[2], Univariate_y[2],\n",
        "                                  alpha=.01,Error=1e-5,iterations=1e3,nodes=40,title=\"Univariate Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy0FZhAYV0zP"
      },
      "outputs": [],
      "source": [
        "Bivariate_y_pred = gradient_descent(Bivariate_X[0], Bivariate_y[0],Bivariate_X[1], Bivariate_y[1],Bivariate_X[2], Bivariate_y[2],\n",
        "                                  alpha=.01,Error=1e-5,iterations=1e3,nodes=20,title=\"Bivariate Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOqT_vsmqSEP"
      },
      "outputs": [],
      "source": [
        "#Plot\n",
        "plt.figure(figsize=(12,12.5))\n",
        "plt.suptitle(\"Actual vs Predicted Output\",fontweight=\"bold\",y=1+1e-2)\n",
        "plt.subplot(3,2,1)\n",
        "plt.title(\"Univariate Train Data\")\n",
        "plt.scatter(Univariate_X[0],Univariate_y[0],label='Actual',alpha=1)\n",
        "plt.scatter(Univariate_X[0],Univariate_y_pred[0],label='Prediction',alpha=1/3)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "\n",
        "plt.subplot(3,2,3)\n",
        "plt.title(\"Univariate Validation Data\")\n",
        "plt.scatter(Univariate_X[1],Univariate_y[1],label='Actual',alpha=1)\n",
        "plt.scatter(Univariate_X[1],Univariate_y_pred[1],label='Prediction',alpha=1/3)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "\n",
        "plt.subplot(3,2,5)\n",
        "plt.title(\"Univariate Test Data\")\n",
        "plt.scatter(Univariate_X[2],Univariate_y[2],label='Actual',alpha=1)\n",
        "plt.scatter(Univariate_X[2],Univariate_y_pred[2],label='Prediction',alpha=1/3)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "\n",
        "ax = plt.subplot(3,2,2,projection=\"3d\")\n",
        "plt.title(\"Bivariate Train Data\")\n",
        "ax.scatter3D(*Bivariate_X[0].T,Bivariate_y_pred[0],label='Prediction',alpha=1)\n",
        "ax.scatter3D(*Bivariate_X[0].T,Bivariate_y[0],label='Actual',alpha=1/3)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "ax.set_zlabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "\n",
        "ax = plt.subplot(3,2,4,projection=\"3d\")\n",
        "plt.title(\"Bivariate Validation Data\")\n",
        "ax.scatter3D(*Bivariate_X[1].T,Bivariate_y_pred[1],label='Prediction',alpha=1)\n",
        "ax.scatter3D(*Bivariate_X[1].T,Bivariate_y[1],label='Actual',alpha=1/3)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "ax.set_zlabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "\n",
        "ax = plt.subplot(3,2,6,projection=\"3d\")\n",
        "plt.title(\"Bivariate Test Data\")\n",
        "ax.scatter3D(*Bivariate_X[2].T,Bivariate_y[2],label='Actual',alpha=1)\n",
        "ax.scatter3D(*Bivariate_X[2].T,Bivariate_y_pred[2],label='Prediction',alpha=1/3)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "ax.set_zlabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_12h3rt6jfI"
      },
      "source": [
        "### 2 hidden layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPaf2oFR6pKq"
      },
      "outputs": [],
      "source": [
        "def init_params(d,node1,node2):\n",
        "    W1 = np.random.normal(0,1,(node1, d))\n",
        "    b1 = np.random.normal(0,1,(node1, 1))\n",
        "    W2 = np.random.normal(0,1,(node2, node1))\n",
        "    b2 = np.random.normal(0,1,(node2, 1))\n",
        "    W3 = np.random.normal(0,1,(1, node2))\n",
        "    b3 = np.random.normal(0,1,(1, 1))\n",
        "    return W1, b1, W2, b2, W3, b3\n",
        "\n",
        "\n",
        "def ReLU(Z):\n",
        "    return np.maximum(Z, 0)\n",
        "\n",
        "def linear(Z):\n",
        "    return Z\n",
        "\n",
        "def forward_prop(W1, b1, W2, b2, W3, b3, X, d):\n",
        "    X = np.reshape(X, (d,1))\n",
        "    Z1 = W1.dot(X) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    A3 = linear(Z3)\n",
        "    return Z1, A1, Z2, A2, Z3, A3\n",
        "\n",
        "def ReLU_deriv(Z):\n",
        "    return Z > 0\n",
        "\n",
        "def backward_prop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y,d,node1,node2):\n",
        "\n",
        "    X = np.reshape(X,(d,1))\n",
        "    Y = np.reshape(Y,(1,1))\n",
        "    A3 = np.reshape(A3,(1,1))\n",
        "    dZ3 = A3 - Y\n",
        "    dZ3 = np.reshape(dZ3, (1, 1))\n",
        "    dW3 = dZ3.dot(A2.T)\n",
        "    db3 = np.sum(dZ3)\n",
        "    dZ2 = W3.T.dot(dZ3) * ReLU_deriv(Z2)\n",
        "    dW2 = dZ2.dot(A1.T)\n",
        "    db2 = np.sum(dZ2)\n",
        "\n",
        "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
        "    dW1 = dZ1.dot(X.T)\n",
        "    db1 = np.sum(dZ1)\n",
        "    return dW1, db1, dW2, db2, dW3, db3\n",
        "\n",
        "\n",
        "def update_params(W1, b1, W2, b2,W3,b3, dW1, db1, dW2, db2,dW3,db3, alpha):\n",
        "    W1 = W1 - alpha * dW1\n",
        "    b1 = b1 - alpha * db1\n",
        "    W2 = W2 - alpha * dW2\n",
        "    b2 = b2 - alpha * db2\n",
        "    W3 = W3 - alpha * dW3\n",
        "    b3 = b3 - alpha * db3\n",
        "    return W1, b1, W2, b2, W3, b3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgcWaVcr8v7c"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, Y,X_validation,Y_validation,X_test,Y_test,iterations,alpha,Error,node1,node2,title):\n",
        "    m = X.shape[0]\n",
        "    try:d = X.shape[1]\n",
        "    except: d = 1\n",
        "    Error_train      = []\n",
        "    Error_validation = []\n",
        "    Error_test       = []\n",
        "    W1, b1, W2, b2,W3,b3 = init_params(d,node1,node2)\n",
        "    for i in range(int(iterations)):\n",
        "      for j in range(m):\n",
        "        k = np.random.randint(0,m-1)\n",
        "        tempX = X[k]\n",
        "        tempY = Y[k]\n",
        "\n",
        "        Z1, A1, Z2, A2,Z3,A3 = forward_prop(W1, b1, W2, b2,W3,b3, tempX,d)\n",
        "        dW1, db1, dW2, db2,dW3,db3 = backward_prop(Z1, A1, Z2, A2,Z3,A3, W1, W2,W3, tempX, tempY,d,node1,node2)\n",
        "        W1, b1, W2, b2,W3,b3 = update_params(W1, b1, W2, b2,W3,b3, dW1, db1, dW2, db2,dW3,db3, alpha)\n",
        "\n",
        "\n",
        "      X1 = np.reshape(X,(m,d))\n",
        "      Z1 = W1.dot(X1.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = ReLU(Z2)\n",
        "      Z3 = W3.dot(A2) + b3\n",
        "      A3 = linear(Z3)\n",
        "      A3 = A3.reshape(len(X1))\n",
        "      y_train_pred = A3\n",
        "      Error_train.append(((A3-Y)**2).sum()/len(X1))\n",
        "\n",
        "      X1 = np.reshape(X_validation,(len(X_validation),d))\n",
        "      Z1 = W1.dot(X1.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = ReLU(Z2)\n",
        "      Z3 = W3.dot(A2) + b3\n",
        "      A3 = linear(Z3)\n",
        "      A3 = A3.reshape(len(X1))\n",
        "      y_validation_pred = A3\n",
        "      Error_validation.append(((A3-Y_validation)**2).sum()/len(X1))\n",
        "\n",
        "      X1 = np.reshape(X_test,(len(X_test),d))\n",
        "      Z1 = W1.dot(X1.T) + b1\n",
        "      A1 = ReLU(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      A2 = ReLU(Z2)\n",
        "      Z3 = W3.dot(A2) + b3\n",
        "      A3 = linear(Z3)\n",
        "      A3 = A3.reshape(len(X1))\n",
        "      y_test_pred = A3\n",
        "      Error_test.append(((A3-Y_test)**2).sum()/len(X1))\n",
        "\n",
        "      if i>1:\n",
        "        if abs(Error_train[-1]-Error_train[-2]) and abs(Error_train[-1]-Error_train[-3]) <Error:\n",
        "          break\n",
        "    #Error Plot\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.suptitle(title,fontweight=\"bold\",y=1+1e-2)\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"Training Error\")\n",
        "    plt.plot(range(1,len(Error_train)+1),Error_train,marker=\".\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Average Error\")\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Validation Error\")\n",
        "    plt.plot(range(1,len(Error_validation)+1),Error_validation,marker=\".\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Average Error\")\n",
        "    plt.grid()\n",
        "    #plt.tight_layout()\n",
        "\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.title(\"Test Error\")\n",
        "    plt.plot(range(1,len(Error_test)+1),Error_test,marker=\".\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Average Error\")\n",
        "    plt.grid()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.suptitle(title,fontweight=\"bold\",y=1+1e-2)\n",
        "    bar_x = (\"Training Data\",\"Validation Data\",\"Test Data\")\n",
        "    bar_y = (Error_train[-1],Error_validation[-1],Error_test[-1])\n",
        "    plt.bar(bar_x,bar_y)\n",
        "    plt.xlabel(\"Data\")\n",
        "    plt.ylabel(\"Mean Square Error\")\n",
        "    plt.grid()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    #Actual vs Predicted Output\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.suptitle(title,fontweight=\"bold\",y=1+1e-2)\n",
        "    #Training Data\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"Training Data\")\n",
        "    plt.scatter(Y,y_train_pred,marker=\".\")\n",
        "    plt.xlabel(\"Target Output\")\n",
        "    plt.ylabel(\"Model Output\")\n",
        "    plt.grid()\n",
        "    #Validation Data\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Validation Data\")\n",
        "    plt.scatter(Y_validation,y_validation_pred,marker=\".\")\n",
        "    plt.xlabel(\"Target Output\")\n",
        "    plt.ylabel(\"Model Output\")\n",
        "    plt.grid()\n",
        "    #Test Data\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.title(\"Training Data\")\n",
        "    plt.scatter(Y_test,y_test_pred,marker=\".\")\n",
        "    plt.xlabel(\"Target Output\")\n",
        "    plt.ylabel(\"Model Output\")\n",
        "    plt.grid()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    #Plot Hidden Layer1\n",
        "    plt.figure(figsize=(12.5,4*node1))\n",
        "    plt.suptitle(title+\"\\nHidden Layer\",fontweight=\"bold\",y=1+1e-2)\n",
        "\n",
        "    p = 1\n",
        "    X1 = np.reshape(X,(m,d))\n",
        "    Z1 = W1.dot(X1.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    A3 = linear(Z3)\n",
        "    A3 = A3.reshape(len(X1))\n",
        "    for j in range(node1):\n",
        "      if d==2:\n",
        "        ax = plt.subplot(node1,3,p,projection=\"3d\")\n",
        "        plt.title(\"Training Data\\nHidden Node\"+str(j+1))\n",
        "        ax.scatter3D(X1.T[0],X1.T[1],A1[j])\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "      elif d==1:\n",
        "        plt.subplot(node1,3,p)\n",
        "        plt.title(\"Training Data\\nHidden Node\"+str(j+1))\n",
        "        plt.scatter(X1,A1[j])\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "\n",
        "    p = 2\n",
        "    X1 = np.reshape(X_validation,(len(X_validation),d))\n",
        "    Z1 = W1.dot(X1.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    A3 = linear(Z3)\n",
        "    A3 = A3.reshape(len(X1))\n",
        "    for j in range(node1):\n",
        "      if d==2:\n",
        "        ax = plt.subplot(node1,3,p,projection=\"3d\")\n",
        "        plt.title(\"Validation Data\\nHidden Node\"+str(j+1))\n",
        "        ax.scatter3D(X1.T[0],X1.T[1],A1[j])\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "      elif d==1:\n",
        "        plt.subplot(node1,3,p)\n",
        "        plt.title(\"Validation Data\\nHidden Node\"+str(j+1))\n",
        "        plt.scatter(X1,A1[j])\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "\n",
        "    p = 3\n",
        "    X1 = np.reshape(X_test,(len(X_test),d))\n",
        "    Z1 = W1.dot(X1.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    A3 = linear(Z3)\n",
        "    A3 = A3.reshape(len(X1))\n",
        "    for j in range(node1):\n",
        "      if d==2:\n",
        "        ax = plt.subplot(node1,3,p,projection=\"3d\")\n",
        "        plt.title(\"Test Data\\nHidden Node\"+str(j+1))\n",
        "        ax.scatter3D(X1.T[0],X1.T[1],A1[j])\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "      elif d==1:\n",
        "        plt.subplot(node1,3,p)\n",
        "        plt.title(\"Test Data\\nHidden Node\"+str(j+1))\n",
        "        plt.scatter(X1,A1[j])\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    #Plot Hidden Layer2\n",
        "    plt.figure(figsize=(12.5,4*node2))\n",
        "    plt.suptitle(title+\"\\nHidden Layer\",fontweight=\"bold\",y=1+1e-2)\n",
        "\n",
        "    p = 1\n",
        "    X1 = np.reshape(X,(m,d))\n",
        "    Z1 = W1.dot(X1.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    A3 = linear(Z3)\n",
        "    A3 = A3.reshape(len(X1))\n",
        "    for j in range(node2):\n",
        "      if d==2:\n",
        "        ax = plt.subplot(node2,3,p,projection=\"3d\")\n",
        "        plt.title(\"Training Data\\nHidden Node\"+str(j+1))\n",
        "        ax.scatter3D(X1.T[0],X1.T[1],A2[j])\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "      elif d==1:\n",
        "        plt.subplot(node2,3,p)\n",
        "        plt.title(\"Training Data\\nHidden Node\"+str(j+1))\n",
        "        plt.scatter(X1,A2[j])\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "\n",
        "    p = 2\n",
        "    X1 = np.reshape(X_validation,(len(X_validation),d))\n",
        "    Z1 = W1.dot(X1.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    A3 = linear(Z3)\n",
        "    A3 = A3.reshape(len(X1))\n",
        "    for j in range(node2):\n",
        "      if d==2:\n",
        "        ax = plt.subplot(node2,3,p,projection=\"3d\")\n",
        "        plt.title(\"Validation Data\\nHidden Node\"+str(j+1))\n",
        "        ax.scatter3D(X1.T[0],X1.T[1],A2[j])\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "      elif d==1:\n",
        "        plt.subplot(node2,3,p)\n",
        "        plt.title(\"Validation Data\\nHidden Node\"+str(j+1))\n",
        "        plt.scatter(X1,A2[j])\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "\n",
        "    p = 3\n",
        "    X1 = np.reshape(X_test,(len(X_test),d))\n",
        "    Z1 = W1.dot(X1.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    A3 = linear(Z3)\n",
        "    A3 = A3.reshape(len(X1))\n",
        "    for j in range(node2):\n",
        "      if d==2:\n",
        "        ax = plt.subplot(node2,3,p,projection=\"3d\")\n",
        "        plt.title(\"Test Data\\nHidden Node\"+str(j+1))\n",
        "        ax.scatter3D(X1.T[0],X1.T[1],A2[j])\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "      elif d==1:\n",
        "        plt.subplot(node2,3,p)\n",
        "        plt.title(\"Test Data\\nHidden Node\"+str(j+1))\n",
        "        plt.scatter(X1,A2[j])\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    #Plot Output Layer\n",
        "    nodes=1\n",
        "    plt.figure(figsize=(12.5,5*nodes))\n",
        "    plt.suptitle(title+\"\\nOutput Layer\",fontweight=\"bold\",y=1+1e-2)\n",
        "\n",
        "    p = 1\n",
        "    X1 = np.reshape(X,(m,d))\n",
        "    Z1 = W1.dot(X1.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    A3 = linear(Z3)\n",
        "    A3 = A3.reshape(len(X1))\n",
        "    for j in range(nodes):\n",
        "      if d==2:\n",
        "        ax = plt.subplot(nodes,3,p,projection=\"3d\")\n",
        "        plt.title(\"Training Data\\nOutput Node\"+str(j+1))\n",
        "        ax.scatter3D(X1.T[0],X1.T[1],A3)\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "      elif d==1:\n",
        "        plt.subplot(nodes,3,p)\n",
        "        plt.title(\"Training Data\\nOutput Node\"+str(j+1))\n",
        "        plt.scatter(X1,A3)\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "\n",
        "    p = 2\n",
        "    X1 = np.reshape(X_validation,(len(X_validation),d))\n",
        "    Z1 = W1.dot(X1.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    A3 = linear(Z3)\n",
        "    A3 = A3.reshape(len(X1))\n",
        "    for j in range(nodes):\n",
        "      if d==2:\n",
        "        ax = plt.subplot(nodes,3,p,projection=\"3d\")\n",
        "        plt.title(\"Validation Data\\nOutput Node\"+str(j+1))\n",
        "        ax.scatter3D(X1.T[0],X1.T[1],A3)\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "      elif d==1:\n",
        "        plt.subplot(nodes,3,p)\n",
        "        plt.title(\"Validation Data\\nOutput Node\"+str(j+1))\n",
        "        plt.scatter(X1,A3)\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "\n",
        "    p = 3\n",
        "    X1 = np.reshape(X_test,(len(X_test),d))\n",
        "    Z1 = W1.dot(X1.T) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    A3 = linear(Z3)\n",
        "    A3 = A3.reshape(len(X1))\n",
        "    for j in range(nodes):\n",
        "      if d==2:\n",
        "        ax = plt.subplot(nodes,3,p,projection=\"3d\")\n",
        "        plt.title(\"Test Data\\nOutput Node\"+str(j+1))\n",
        "        ax.scatter3D(X1.T[0],X1.T[1],A3)\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\")\n",
        "        ax.set_zlabel(\"Activation for Neuron \"+str(j+1))\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "      elif d==1:\n",
        "        plt.subplot(nodes,3,p)\n",
        "        plt.title(\"Test Data\\nOutput Node\"+str(j+1))\n",
        "        plt.scatter(X1,A3)\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.grid()\n",
        "        p+=3\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return (y_train_pred,y_validation_pred,y_test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lRIUTt_9k24"
      },
      "outputs": [],
      "source": [
        "Univariate_y_pred = gradient_descent(Univariate_X[0], Univariate_y[0],Univariate_X[1], Univariate_y[1],Univariate_X[2], Univariate_y[2],\n",
        "                                  alpha=.001,Error=1e-5,iterations=1e3,node1=40,node2=20,title=\"Univariate Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OPKkMcTEION"
      },
      "outputs": [],
      "source": [
        "Bivariate_y_pred = gradient_descent(Bivariate_X[0], Bivariate_y[0],Bivariate_X[1], Bivariate_y[1],Bivariate_X[2], Bivariate_y[2],\n",
        "                                  alpha=.001,Error=1e-5,iterations=1e3,node1=20,node2=10,title=\"Bivariate Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrSukFMIGJFy"
      },
      "outputs": [],
      "source": [
        "#Plot\n",
        "plt.figure(figsize=(12,12.5))\n",
        "plt.suptitle(\"Actual vs Predicted Output\",fontweight=\"bold\",y=1+1e-2)\n",
        "plt.subplot(3,2,1)\n",
        "plt.title(\"Univariate Train Data\")\n",
        "plt.scatter(Univariate_X[0],Univariate_y[0],label='Actual',alpha=1)\n",
        "plt.scatter(Univariate_X[0],Univariate_y_pred[0],label='Prediction',alpha=1/3)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "\n",
        "plt.subplot(3,2,3)\n",
        "plt.title(\"Univariate Validation Data\")\n",
        "plt.scatter(Univariate_X[1],Univariate_y[1],label='Actual',alpha=1)\n",
        "plt.scatter(Univariate_X[1],Univariate_y_pred[1],label='Prediction',alpha=1/3)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "\n",
        "plt.subplot(3,2,5)\n",
        "plt.title(\"Univariate Test Data\")\n",
        "plt.scatter(Univariate_X[2],Univariate_y[2],label='Actual',alpha=1)\n",
        "plt.scatter(Univariate_X[2],Univariate_y_pred[2],label='Prediction',alpha=1/3)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "\n",
        "ax = plt.subplot(3,2,2,projection=\"3d\")\n",
        "plt.title(\"Bivariate Train Data\")\n",
        "ax.scatter3D(*Bivariate_X[0].T,Bivariate_y_pred[0],label='Prediction',alpha=1)\n",
        "ax.scatter3D(*Bivariate_X[0].T,Bivariate_y[0],label='Actual',alpha=1/3)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "ax.set_zlabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "\n",
        "ax = plt.subplot(3,2,4,projection=\"3d\")\n",
        "plt.title(\"Bivariate Validation Data\")\n",
        "ax.scatter3D(*Bivariate_X[1].T,Bivariate_y_pred[1],label='Prediction',alpha=1)\n",
        "ax.scatter3D(*Bivariate_X[1].T,Bivariate_y[1],label='Actual',alpha=1/3)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "ax.set_zlabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "\n",
        "ax = plt.subplot(3,2,6,projection=\"3d\")\n",
        "plt.title(\"Bivariate Test Data\")\n",
        "ax.scatter3D(*Bivariate_X[2].T,Bivariate_y[2],label='Actual',alpha=1)\n",
        "ax.scatter3D(*Bivariate_X[2].T,Bivariate_y_pred[2],label='Prediction',alpha=1/3)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "ax.set_zlabel(\"y\")\n",
        "plt.grid()\n",
        "plt.legend(loc=1)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
